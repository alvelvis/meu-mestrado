{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isfile(\"estrutura_ud.py\"):\n",
    "    os.system(\"wget https://raw.githubusercontent.com/alvelvis/ACDC-UD/master/estrutura_ud.py\")\n",
    "if not os.path.isfile(\"tokenization.py\"):\n",
    "    os.system(\"wget https://raw.githubusercontent.com/alvelvis/ACDC-UD/master/tokenization.py\")\n",
    "import estrutura_ud\n",
    "import tokenization\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "file = \"Petroles_3_golden.conllu\"\n",
    "release_file = \"PetroGold.conllu\"\n",
    "\n",
    "def change_col(col, token, value):\n",
    "    col = token.__dict__[col].split(\"|\")\n",
    "    col = [x for x in col if not x.startswith(value.split(\"=\")[0]) and x != \"_\"] + [value]\n",
    "    col = \"|\".join(sorted(col))\n",
    "    return col\n",
    "\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "def count_tokens(corpus, i=0):\n",
    "    for sentence in corpus.sentences.values():\n",
    "        i += len([x for x in sentence.tokens if not '-' in x.id])\n",
    "    return i\n",
    "\n",
    "def count_words(corpus, i=0):\n",
    "    for sentence in corpus.sentences.values():\n",
    "        i += len([x for x in sentence.tokens if not '-' in x.id and x.upos != \"PUNCT\"])\n",
    "    return i\n",
    "\n",
    "def count_documents(corpus):\n",
    "    return len(set([x.rsplit(\"-\", 1)[0] for x in corpus.sentences]))\n",
    "\n",
    "def count_entities(corpus, i=0):\n",
    "    for sentence in corpus.sentences.values():\n",
    "        i += len([x for x in sentence.tokens if 'B=' in x.deps])\n",
    "    return i\n",
    "\n",
    "def count_all(label, corpus, stats=\"\"):\n",
    "    if not stats:\n",
    "        stats = {'sentences': {}, 'tokens': {}, 'entities': {}, 'documents': {}, 'words': {}}\n",
    "    stats['sentences'][label] = len(corpus.sentences)\n",
    "    stats['tokens'][label] = count_tokens(corpus)\n",
    "    stats['documents'][label] = count_documents(corpus)\n",
    "    stats['entities'][label] = count_entities(corpus)\n",
    "    stats['words'][label] = count_words(corpus)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COSMO sem root e ciclo\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "for sentence in corpus.sentences.values():\n",
    "    if not any(x.deprel == \"root\" for x in sentence.tokens):\n",
    "        sentence.tokens[0].misc = \"SEM ROOT\"\n",
    "    elif sentence.to_str().count(\"\\troot\\t\") > 1:\n",
    "        for token in sentence.tokens:\n",
    "            if token.deprel == \"root\":\n",
    "                token.misc = \"MAIS DE 1 ROOT\"\n",
    "    else:\n",
    "        for token in sentence.tokens:\n",
    "            if (\n",
    "                token.dephead != \"0\" and (\n",
    "                    token.dephead == token.head_token.dephead or \n",
    "                    token.dephead == token.id or \n",
    "                    token.id == token.head_token.dephead or \n",
    "                    (token.head_token.dephead != \"0\" and token.head_token.head_token.dephead == token.id) or\n",
    "                    (token.head_token.dephead != \"0\" and token.head_token.head_token.dephead != \"0\" and token.head_token.head_token.head_token.dephead == token.id)\n",
    "                )):\n",
    "                token.misc = \"CICLO\"\n",
    "            if token.dephead == \"0\" and token.deprel != \"root\":\n",
    "                token.misc = \"root\"\n",
    "            if token.dephead != \"0\" and token.deprel == \"root\":\n",
    "                token.misc = \"root\"\n",
    "\n",
    "file_name = file.rsplit(\"/\", 1)[1] if '/' in file else file\n",
    "corpus.save(\"regras.conllu\")\n",
    "os.system(\"python3 ../../conllu-merge-resolver/cosmo.py {} regras.conllu '.*'\".format(file))\n",
    "os.system(\"cp {} antes-{}\".format(file, file_name))\n",
    "os.system(\"rm regras-correcao.zip; zip regras-correcao.zip antes-{} regras.conllu\".format(file_name))\n",
    "os.remove(\"antes-{}\".format(file_name))\n",
    "os.remove(\"regras.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correções de formato: ordem alfabética das feats, =, e deprels que não podem ter filho\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "for sentence in corpus.sentences.values():\n",
    "    for token in sentence.tokens:\n",
    "        if '</' in token.to_str():\n",
    "            token.misc = \"</\"\n",
    "        if ' ' in token.to_str():\n",
    "            token.misc = \"espaço em branco\"\n",
    "        if not token.feats.strip():\n",
    "            token.feats = \"_\"\n",
    "        if token.feats != \"_\":\n",
    "            try:\n",
    "                parts = [x.split('=') for x in token.feats.split(\"|\")]\n",
    "                parts = dict(parts)\n",
    "            except ValueError:\n",
    "                token.misc = \"feats com erro\"\n",
    "            token.feats = \"|\".join(sorted(token.feats.split(\"|\"), key=lambda x: x.lower()))\n",
    "    \n",
    "    cant_have_children = \"flat:name compound punct fixed aux cop case mark punct\".split()\n",
    "    for token in sentence.tokens:\n",
    "        if token.deprel not in \"fixed compound\".split() and token.head_token.deprel in cant_have_children:\n",
    "            token.dephead = token.head_token.dephead if token.head_token.head_token.deprel not in cant_have_children else token.head_token.head_token.dephead\n",
    "\n",
    "file_name = file.rsplit(\"/\", 1)[1] if '/' in file else file\n",
    "corpus.save(\"regras.conllu\")\n",
    "os.system(\"python3 ../../conllu-merge-resolver/cosmo.py {} regras.conllu '.*'\".format(file))\n",
    "os.system(\"cp {} antes-{}\".format(file, file_name))\n",
    "os.system(\"rm regras-correcao.zip; zip regras-correcao.zip antes-{} regras.conllu\".format(file_name))\n",
    "os.remove(\"antes-{}\".format(file_name))\n",
    "os.remove(\"regras.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT IGUAIS\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "texts = {}\n",
    "for sentid, sentence in corpus.sentences.items():\n",
    "    text = sentence.text\n",
    "    if not text in texts:\n",
    "        texts[text] = []\n",
    "    texts[text].append(sentid)\n",
    "\n",
    "with open(\"frases-repetidas.txt\", \"w+\") as f:\n",
    "    for text in texts:\n",
    "        if len(texts[text]) >= 2:\n",
    "            f.write(\"{}\\n{}\\n\\n\".format(text, \"|\".join(texts[text])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text contrações\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "changed_sentences = []\n",
    "for sentence in corpus.sentences.values():\n",
    "    if not sentence.text.endswith(sentence.tokens[-1].word):\n",
    "        contractions = [[int(x.id.split(\"-\")[0]), int(x.id.split(\"-\")[1]), x.word] for x in sentence.tokens if '-' in x.id]\n",
    "        new_text = [x.word for x in sentence.tokens if not '-' in x.id]\n",
    "        for contraction in reversed(contractions):\n",
    "            start = contraction[0]\n",
    "            end = contraction[1]\n",
    "            word = contraction[2]\n",
    "            for i in reversed(range(start-1, end)):\n",
    "                new_text.pop(i)\n",
    "            new_text.insert(start-1, word)\n",
    "        new_text = \" \".join(new_text)\n",
    "        sentence.metadados['text'] = new_text\n",
    "        changed_sentences.append(sentence.sent_id)\n",
    "\n",
    "print(\"|\".join(changed_sentences))\n",
    "\n",
    "corpus.save(\"regras.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminando pontuação duplicada\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "changed_sentences = []\n",
    "for sentid, sentence in corpus.sentences.items():\n",
    "    if sentence.tokens[-1].word == \".\" and sentence.tokens[-2].word == \".\":\n",
    "        sentence = tokenization.addToken(corpus, sentid, 'rm', sentence.tokens[-1].id).sentences[sentid]\n",
    "        sentence.tokens[-1].dephead = [x for x in sentence.tokens if x.deprel == \"root\"][0].id\n",
    "        changed_sentences.append(sentence.sent_id)\n",
    "    if sentence.tokens[-1].word == \"..\":\n",
    "        sentence.tokens[-1].word = \".\"\n",
    "        sentence.tokens[-1].lemma = \".\"\n",
    "        sentence.tokens[-1].dephead = [x for x in sentence.tokens if x.deprel == \"root\"][0].id\n",
    "        changed_sentences.append(sentence.sent_id)\n",
    "    if sentence.text.endswith(\"..\"):\n",
    "        sentence.metadados['text'] = sentence.text[:-1]\n",
    "        changed_sentences.append(sentence.sent_id)\n",
    "\n",
    "print(\"|\".join(changed_sentences))\n",
    "\n",
    "corpus.save(\"regras.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens com ..\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "for sentid, sentence in corpus.sentences.items():\n",
    "    if sentence.tokens[-1].word.endswith(\"..\") and (sentence.tokens[-1].upos == \"PUNCT\" or sentence.tokens[-1].deprel == \"punct\"):\n",
    "        sentence.tokens[-1].upos = \"EPA\"\n",
    "        sentence.tokens[-1].misc = sentence.tokens[-1].misc.replace(\"tokenização\", \"\").replace(\"||\", \"|\")\n",
    "        if not sentence.tokens[-1].misc:\n",
    "            sentence.tokens[-1].misc = \"_\"\n",
    "\n",
    "corpus.save(\"regras.conllu\")\n",
    "os.system(\"python3 ../../conllu-merge-resolver/cosmo.py {} regras.conllu '.*'\".format(file))\n",
    "os.system(\"cp {} antes-{}\".format(file, file_name))\n",
    "os.system(\"rm regras-correcao.zip; zip regras-correcao.zip antes-{} regras.conllu\".format(file_name))\n",
    "os.remove(\"antes-{}\".format(file_name))\n",
    "os.remove(\"regras.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens com .. parte 2\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "changed_sentences = []\n",
    "for sentid, sentence in corpus.sentences.items():\n",
    "    if sentence.tokens[-1].word.endswith(\"..\"):\n",
    "        sentence = tokenization.addToken(corpus, sentid, 'add', str(int(sentence.tokens[-1].id)+1), new_tokens=[]).sentences[sentid]\n",
    "        sentence.tokens[-1].dephead = [x for x in sentence.tokens if x.deprel == \"root\"][0].id\n",
    "        sentence.tokens[-1].lemma = \".\"\n",
    "        sentence.tokens[-1].word = \".\"\n",
    "        sentence.tokens[-1].deprel = \"punct\"\n",
    "        sentence.tokens[-1].upos = \"PUNCT\"\n",
    "        sentence.tokens[-2].word = sentence.tokens[-2].word[:-2]\n",
    "        sentence.tokens[-2].lemma = sentence.tokens[-2].lemma[:-2]\n",
    "        changed_sentences.append(sentence.sent_id)\n",
    "\n",
    "print(\"|\".join(changed_sentences))\n",
    "\n",
    "corpus.save(\"regras.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminando frases\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "old_sentences = set(corpus.sentences.keys())\n",
    "corpus.sentences = {x: corpus.sentences[x] for x in corpus.sentences if not x.startswith(\"258-\")}\n",
    "corpus.sentences = {x: corpus.sentences[x] for x in corpus.sentences if not any(y in corpus.sentences[x].to_str() for y in \"sentenciação\".split())}\n",
    "corpus.sentences = {x: corpus.sentences[x] for x in corpus.sentences if not any(y in corpus.sentences[x].to_str() for y in \"tokenização\".split())}\n",
    "corpus.sentences = {x: corpus.sentences[x] for x in corpus.sentences if not any(y in corpus.sentences[x].to_str() for y in \"eliminar\".split())}\n",
    "new_sentences = set(corpus.sentences.keys())\n",
    "\n",
    "if new_sentences != old_sentences:\n",
    "    print(len(new_sentences))\n",
    "    print(len(old_sentences))\n",
    "\n",
    "corpus.save(\"regras.conllu\")\n",
    "if new_sentences != old_sentences:\n",
    "    print(\"Atualize frases: cp regras.conllu {}\".format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list sentences removed\n",
    "pre_commit = \"48efd0a9d2092cd8f4dd17c0eda7bcc85522f02c\"\n",
    "pos_commit = \"master\"\n",
    "\n",
    "os.system(\"git checkout {}\".format(pre_commit))\n",
    "corpus_pre = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_pre.load(file)\n",
    "\n",
    "os.system(\"git checkout {}\".format(pos_commit))\n",
    "corpus_pos = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_pos.load(file)\n",
    "\n",
    "stats = count_all(\"pré\", corpus_pre)\n",
    "stats = count_all(\"pós\", corpus_pos, stats)\n",
    "\n",
    "sent_ids = set(corpus_pre.sentences)\n",
    "sent_ids_removed = sent_ids - set(corpus_pos.sentences)\n",
    "print(\"|\".join(sent_ids_removed))\n",
    "\n",
    "pprint.pprint(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remover sema, xpos, misc, nmod:appos (só para release)\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "for sentid, sentence in corpus.sentences.items():\n",
    "    for token in sentence.tokens:\n",
    "        token.deps = \"_\"\n",
    "        token.xpos = \"_\"\n",
    "        token.misc = \"_\"\n",
    "        if token.deprel == \"nmod:appos\":\n",
    "            token.deprel = \"nmod\"\n",
    "        if token.deprel == \"expl:impers\":\n",
    "            token.deprel = \"expl\"\n",
    "\n",
    "corpus.save(release_file)\n",
    "\n",
    "for sentid, sentence in corpus.sentences.items():\n",
    "    for token in sentence.tokens:\n",
    "        if token.deprel == \"obl:arg\":\n",
    "            token.deprel = \"obl\"\n",
    "\n",
    "corpus.save(\"experimento-obl-arg.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATE.PY => erros de formato devem ser corrigidos\n",
    "! ~/Documentos/Interrogat-rio/www/cgi-bin/tools/validate.py PetroGold.conllu --lang=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPARAR TRAIN E TEST\n",
    "files = [release_file, \"experimento-obl-arg.conllu\"]\n",
    "\n",
    "sentences = {}\n",
    "get_sentences_for_partitions = {}\n",
    "\n",
    "# comment this line if randomly generate partitions\n",
    "get_sentences_for_partitions = {\n",
    "    \"dev\": \"PetroGold-v1/petrogold-dev.conllu\",\n",
    "    \"train\": \"PetroGold-v1/petrogold-train.conllu\",\n",
    "    \"test\": \"PetroGold-v1/petrogold-test.conllu\"\n",
    "    }\n",
    "\n",
    "if get_sentences_for_partitions:\n",
    "    for partition in get_sentences_for_partitions:\n",
    "        corpus = estrutura_ud.Corpus()\n",
    "        corpus.load(get_sentences_for_partitions[partition])\n",
    "        sentences[partition] = set(corpus.sentences)        \n",
    "\n",
    "for file in files:\n",
    "    corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "    corpus.load(file)\n",
    "\n",
    "    if not sentences:\n",
    "        all_sentences = set(corpus.sentences)\n",
    "        shuffled = list(corpus.sentences)\n",
    "        random.shuffle(shuffled)\n",
    "        sentences[\"test\"] = set(shuffled[:round(0.05*len(all_sentences))])\n",
    "        sentences[\"dev\"] = set([x for x in shuffled if x not in sentences['test']][:round(0.05*len(all_sentences))])\n",
    "        sentences[\"train\"] = all_sentences - sentences[\"test\"] - sentences[\"dev\"]\n",
    "    assert not sentences[\"train\"].intersection(sentences[\"test\"])\n",
    "    assert not sentences[\"train\"].intersection(sentences[\"dev\"])\n",
    "    assert not sentences[\"test\"].intersection(sentences[\"dev\"])\n",
    "\n",
    "    train = []\n",
    "    test = []\n",
    "    dev = []\n",
    "    for sentid, sentence in corpus.sentences.items():\n",
    "        if sentid in sentences[\"test\"]:\n",
    "            test.append(sentence.to_str())\n",
    "        if sentid in sentences[\"train\"]:\n",
    "            train.append(sentence.to_str())\n",
    "        if sentid in sentences[\"dev\"]:\n",
    "            dev.append(sentence.to_str())\n",
    "\n",
    "    corpus_train = estrutura_ud.Corpus(recursivo=True)\n",
    "    corpus_test = estrutura_ud.Corpus(recursivo=True)\n",
    "    corpus_dev = estrutura_ud.Corpus(recursivo=True)\n",
    "    corpus_train.build(\"\\n\\n\".join(train) + \"\\n\\n\")\n",
    "    corpus_test.build(\"\\n\\n\".join(test) + \"\\n\\n\")\n",
    "    corpus_dev.build(\"\\n\\n\".join(dev) + \"\\n\\n\")\n",
    "\n",
    "    stats = count_all(\"train\", corpus_train)\n",
    "    stats = count_all(\"test\", corpus_test, stats)\n",
    "    stats = count_all(\"dev\", corpus_dev, stats)\n",
    "    pprint.pprint(stats)\n",
    "\n",
    "    corpus_train.save(\"{}-train.conllu\".format(file.rsplit(\".\", 1)[0]))\n",
    "    corpus_test.save(\"{}-test.conllu\".format(file.rsplit(\".\", 1)[0]))\n",
    "    corpus_dev.save(\"{}-dev.conllu\".format(file.rsplit(\".\", 1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TREINAR COM TRAIN E DEV\n",
    "! ./udpipe-1.2.0 --train --tokenizer=none \"PetroGold.udpipe\" \"PetroGold-train.conllu\" \"PetroGold-dev.conllu\"\n",
    "! ./udpipe-1.2.0 --train --tokenizer=none \"experimento-obl-arg.udpipe\" \"experimento-obl-arg-train.conllu\" \"experimento-obl-arg-dev.conllu\"\n",
    "#! ./udpipe-1.2.0 --train --tokenizer=none \"bosque-ud-2.8/bosque2.8.udpipe\" \"bosque-ud-2.8/pt_bosque-ud-train.conllu\" \"bosque-ud-2.8/pt_bosque-ud-dev.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE PETRO3\n",
    "! python3 ~/Documentos/ACDC-UD/tokenizar_conllu.py \"PetroGold-test.conllu\" \"PetroGold-test-tokenizado.txt\"\n",
    "! python3 ~/Documentos/ACDC-UD/udpipe_vertical.py \"PetroGold.udpipe\" \"PetroGold-test-tokenizado.txt\" \"PetroGold-test-udpipe.conllu\"\n",
    "! python3 ~/Documentos/ACDC-UD/conll18_ud_eval.py -v \"PetroGold-test.conllu\" \"PetroGold-test-udpipe.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE EXPERIMENTO OBL ARG\n",
    "! python3 ~/Documentos/ACDC-UD/tokenizar_conllu.py \"experimento-obl-arg-test.conllu\" \"experimento-obl-arg-test-tokenizado.txt\"\n",
    "! python3 ~/Documentos/ACDC-UD/udpipe_vertical.py \"experimento-obl-arg.udpipe\" \"experimento-obl-arg-test-tokenizado.txt\" \"experimento-obl-arg-test-udpipe.conllu\"\n",
    "! python3 ~/Documentos/ACDC-UD/conll18_ud_eval.py -v \"experimento-obl-arg-test.conllu\" \"experimento-obl-arg-test-udpipe.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE BOSQUE2.8\n",
    "#! python3 ~/Documentos/ACDC-UD/tokenizar_conllu.py \"bosque-ud-2.8/pt_bosque-ud-test.conllu\" \"bosque-ud-2.8/pt_bosque-ud-test-tokenizado.txt\"\n",
    "#! python3 ~/Documentos/ACDC-UD/udpipe_vertical.py \"bosque-ud-2.8/bosque2.8.udpipe\" \"bosque-ud-2.8/pt_bosque-ud-test-tokenizado.txt\" \"bosque-ud-2.8/pt_bosque-ud-test-udpipe.conllu\"\n",
    "! python3 ~/Documentos/ACDC-UD/conll18_ud_eval.py -v \"bosque-ud-2.8/pt_bosque-ud-test.conllu\" \"bosque-ud-2.8/pt_bosque-ud-test-udpipe.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNT MODIFICATIONS\n",
    "file = \"Petroles_3_golden.conllu\"\n",
    "file_stanza = \"Petroles_3_stanza.conllu\"\n",
    "file_udpipe = \"Petroles_3_udpipe.conllu\"\n",
    "post_changes = \"master\"\n",
    "\n",
    "os.system(\"git checkout {}\".format(post_changes))\n",
    "corpus_pos = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_pos.load(file)\n",
    "\n",
    "corpus_stanza = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_stanza.load(file_stanza)\n",
    "corpus_udpipe = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_udpipe.load(file_udpipe)\n",
    "\n",
    "sentences_that_matter = list(corpus_pos.sentences.keys())\n",
    "print(\"Sentences that matter: {}\".format(len(sentences_that_matter)))\n",
    "\n",
    "tokens_changed = set()\n",
    "tokens_changed_lemma = set()\n",
    "tokens_changed_deprel = set()\n",
    "tokens_changed_dephead = set()\n",
    "tokens_changed_upos = set()\n",
    "sentences_changed = set()\n",
    "udpipe_correct = set()\n",
    "changed_lemmas = {}\n",
    "all_tokens = 0\n",
    "all_sentences = 0\n",
    "cm = set()\n",
    "for sentid in sentences_that_matter:\n",
    "    for t, token in enumerate(corpus_pos.sentences[sentid].tokens):\n",
    "        if not '-' in token.id and token.upos != \"PUNCT\":\n",
    "            all_tokens += 1\n",
    "            try:\n",
    "                token_pos = corpus_pos.sentences[sentid].tokens[t]\n",
    "                token_stanza = corpus_stanza.sentences[sentid].tokens[t]\n",
    "                token_udpipe = corpus_udpipe.sentences[sentid].tokens[t]\n",
    "            except:\n",
    "                continue\n",
    "            important_cols_pos = [token_pos.lemma, token_pos.upos, token_pos.feats, token_pos.dephead, token_pos.deprel]\n",
    "            important_cols_stanza = [token_stanza.lemma, token_stanza.upos, token_stanza.feats, token_stanza.dephead, token_stanza.deprel]\n",
    "            important_cols_udpipe = [token_udpipe.lemma, token_udpipe.upos, token_udpipe.feats, token_udpipe.dephead, token_udpipe.deprel]\n",
    "            if token_stanza.deprel != token_udpipe.deprel:\n",
    "                cm.add(\"{}-{}\".format(sentid, t))\n",
    "                if token_udpipe.deprel == token_pos.deprel:\n",
    "                    udpipe_correct.add(\"{}-{}\".format(sentid, t))\n",
    "            if important_cols_pos != important_cols_stanza:\n",
    "                #if token_pos.deprel == \"obl:arg\" and token_stanza.deprel in [\"obj\", \"iobj\"]:\n",
    "                    #continue\n",
    "                tokens_changed.add(\"{}-{}\".format(sentid, t))\n",
    "                if not sentid in sentences_changed:\n",
    "                    sentences_changed.add(sentid)\n",
    "                if token_pos.lemma != token_stanza.lemma:\n",
    "                    tokens_changed_lemma.add(\"{}-{}\".format(sentid, t))\n",
    "                    if token_pos.lemma.lower() != token_stanza.lemma.lower() and token_pos.upos == \"VERB\":\n",
    "                        if not token_pos.lemma.lower() in changed_lemmas:\n",
    "                            changed_lemmas[token_pos.lemma.lower()] = {}\n",
    "                        if not token_stanza.lemma.lower() in changed_lemmas[token_pos.lemma.lower()]:\n",
    "                            changed_lemmas[token_pos.lemma.lower()][token_stanza.lemma.lower()] = 0\n",
    "                        changed_lemmas[token_pos.lemma.lower()][token_stanza.lemma.lower()] += 1\n",
    "                if token_pos.upos != token_stanza.upos:\n",
    "                    tokens_changed_upos.add(\"{}-{}\".format(sentid, t))\n",
    "                if token_pos.dephead != token_stanza.dephead:\n",
    "                    tokens_changed_dephead.add(\"{}-{}\".format(sentid, t))\n",
    "                if token_pos.deprel != token_stanza.deprel:\n",
    "                    tokens_changed_deprel.add(\"{}-{}\".format(sentid, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\n",
    "ocorrencias = 0\n",
    "c = 1\n",
    "for lemma in sorted(changed_lemmas):\n",
    "    ocorrencias += sum([changed_lemmas[lemma][x] for x in changed_lemmas[lemma]])\n",
    "    txt += \"{} ({}) {}\".format(\", \".join(list(changed_lemmas[lemma].keys())), lemma, \"& \" if c%4 else \"\\\\\\\\\\n\")\n",
    "    c += 1\n",
    "print(txt)\n",
    "\n",
    "print(\"Lemas corrigidos: {}\".format(len(changed_lemmas)))\n",
    "print(\"Ocorrências: {}\".format(ocorrencias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAY INTERSEÇÕES\n",
    "print(\"EXCEPT PUNCT -- Tokens changed: {} / All_tokens: {}\".format(len(tokens_changed), all_tokens))\n",
    "print(\"Sentences changed: {} / All_sentences: {}\".format(len(sentences_changed), len(sentences_that_matter)))\n",
    "print(\"CM: {} / Intersection: {} / UDPIPE_CORRECT: {}\".format(len(cm), len(tokens_changed.intersection(cm)), len(udpipe_correct)))\n",
    "print(len(tokens_changed_lemma))\n",
    "print(len(tokens_changed_upos))\n",
    "print(len(tokens_changed_dephead))\n",
    "print(len(tokens_changed_deprel))\n",
    "print(\"\")\n",
    "print(len(tokens_changed_lemma.intersection(cm)))\n",
    "print(len(tokens_changed_upos.intersection(cm)))\n",
    "print(len(tokens_changed_dephead.intersection(cm)))\n",
    "print(len(tokens_changed_deprel.intersection(cm)))\n",
    "print(\"\")\n",
    "print(len(tokens_changed_lemma.intersection(tokens_changed_upos)))\n",
    "print(len(tokens_changed_lemma.intersection(tokens_changed_upos).intersection(cm)))\n",
    "print(len(tokens_changed_lemma.intersection(tokens_changed_deprel)))\n",
    "print(len(tokens_changed_lemma.intersection(tokens_changed_deprel).intersection(cm)))\n",
    "print(len(tokens_changed_upos.intersection(tokens_changed_deprel)))\n",
    "print(len(tokens_changed_upos.intersection(tokens_changed_deprel).intersection(cm)))\n",
    "print(len(tokens_changed_dephead.intersection(tokens_changed_deprel)))\n",
    "print(len(tokens_changed_dephead.intersection(tokens_changed_deprel).intersection(cm)))\n",
    "print(len(tokens_changed_lemma - tokens_changed_upos - tokens_changed_dephead - tokens_changed_deprel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
