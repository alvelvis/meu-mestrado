{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isfile(\"estrutura_ud.py\"):\n",
    "    os.system(\"wget https://raw.githubusercontent.com/alvelvis/ACDC-UD/master/estrutura_ud.py\")\n",
    "if not os.path.isfile(\"tokenization.py\"):\n",
    "    os.system(\"wget https://raw.githubusercontent.com/alvelvis/ACDC-UD/master/tokenization.py\")\n",
    "import estrutura_ud\n",
    "import tokenization\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "file = \"Petroles_3_golden.conllu\"\n",
    "release_file = \"PetroGold.conllu\"\n",
    "\n",
    "def change_col(col, token, value):\n",
    "    col = token.__dict__[col].split(\"|\")\n",
    "    col = [x for x in col if not x.startswith(value.split(\"=\")[0]) and x != \"_\"] + [value]\n",
    "    col = \"|\".join(sorted(col))\n",
    "    return col\n",
    "\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "def count_tokens(corpus, i=0):\n",
    "    for sentence in corpus.sentences.values():\n",
    "        i += len([x for x in sentence.tokens if not '-' in x.id])\n",
    "    return i\n",
    "\n",
    "def count_words(corpus, i=0):\n",
    "    for sentence in corpus.sentences.values():\n",
    "        i += len([x for x in sentence.tokens if not '-' in x.id and x.upos != \"PUNCT\"])\n",
    "    return i\n",
    "\n",
    "def count_documents(corpus):\n",
    "    return len(set([x.rsplit(\"-\", 1)[0] for x in corpus.sentences]))\n",
    "\n",
    "def count_entities(corpus, i=0):\n",
    "    for sentence in corpus.sentences.values():\n",
    "        i += len([x for x in sentence.tokens if 'B=' in x.deps])\n",
    "    return i\n",
    "\n",
    "def count_all(label, corpus, stats=\"\"):\n",
    "    if not stats:\n",
    "        stats = {'sentences': {}, 'tokens': {}, 'entities': {}, 'documents': {}, 'words': {}}\n",
    "    stats['sentences'][label] = len(corpus.sentences)\n",
    "    stats['tokens'][label] = count_tokens(corpus)\n",
    "    stats['documents'][label] = count_documents(corpus)\n",
    "    stats['entities'][label] = count_entities(corpus)\n",
    "    stats['words'][label] = count_words(corpus)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COSMO sem root e ciclo\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "for sentence in corpus.sentences.values():\n",
    "    if not any(x.deprel == \"root\" for x in sentence.tokens):\n",
    "        sentence.tokens[0].misc = \"SEM ROOT\"\n",
    "    elif sentence.to_str().count(\"\\troot\\t\") > 1:\n",
    "        for token in sentence.tokens:\n",
    "            if token.deprel == \"root\":\n",
    "                token.misc = \"MAIS DE 1 ROOT\"\n",
    "    else:\n",
    "        for token in sentence.tokens:\n",
    "            if (\n",
    "                token.dephead != \"0\" and (\n",
    "                    token.dephead == token.head_token.dephead or \n",
    "                    token.dephead == token.id or \n",
    "                    token.id == token.head_token.dephead or \n",
    "                    (token.head_token.dephead != \"0\" and token.head_token.head_token.dephead == token.id) or\n",
    "                    (token.head_token.dephead != \"0\" and token.head_token.head_token.dephead != \"0\" and token.head_token.head_token.head_token.dephead == token.id)\n",
    "                )):\n",
    "                token.misc = \"CICLO\"\n",
    "            if token.dephead == \"0\" and token.deprel != \"root\":\n",
    "                token.misc = \"root\"\n",
    "            if token.dephead != \"0\" and token.deprel == \"root\":\n",
    "                token.misc = \"root\"\n",
    "\n",
    "file_name = file.rsplit(\"/\", 1)[1] if '/' in file else file\n",
    "corpus.save(\"regras.conllu\")\n",
    "os.system(\"python3 ../../conllu-merge-resolver/cosmo.py {} regras.conllu '.*'\".format(file))\n",
    "os.system(\"cp {} antes-{}\".format(file, file_name))\n",
    "os.system(\"rm regras-correcao.zip; zip regras-correcao.zip antes-{} regras.conllu\".format(file_name))\n",
    "os.remove(\"antes-{}\".format(file_name))\n",
    "os.remove(\"regras.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correções de formato: ordem alfabética das feats, =, e deprels que não podem ter filho\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "for sentence in corpus.sentences.values():\n",
    "    for token in sentence.tokens:\n",
    "        if '</' in token.to_str():\n",
    "            token.misc = \"</\"\n",
    "        if ' ' in token.to_str():\n",
    "            token.misc = \"espaço em branco\"\n",
    "        if not token.feats.strip():\n",
    "            token.feats = \"_\"\n",
    "        if token.feats != \"_\":\n",
    "            try:\n",
    "                parts = [x.split('=') for x in token.feats.split(\"|\")]\n",
    "                parts = dict(parts)\n",
    "            except ValueError:\n",
    "                token.misc = \"feats com erro\"\n",
    "            token.feats = \"|\".join(sorted(token.feats.split(\"|\"), key=lambda x: x.lower()))\n",
    "    \n",
    "    cant_have_children = \"flat:name compound punct fixed aux cop case mark punct\".split()\n",
    "    for token in sentence.tokens:\n",
    "        if token.deprel not in \"fixed compound\".split() and token.head_token.deprel in cant_have_children:\n",
    "            token.dephead = token.head_token.dephead if token.head_token.head_token.deprel not in cant_have_children else token.head_token.head_token.dephead\n",
    "\n",
    "file_name = file.rsplit(\"/\", 1)[1] if '/' in file else file\n",
    "corpus.save(\"regras.conllu\")\n",
    "os.system(\"python3 ../../conllu-merge-resolver/cosmo.py {} regras.conllu '.*'\".format(file))\n",
    "os.system(\"cp {} antes-{}\".format(file, file_name))\n",
    "os.system(\"rm regras-correcao.zip; zip regras-correcao.zip antes-{} regras.conllu\".format(file_name))\n",
    "os.remove(\"antes-{}\".format(file_name))\n",
    "os.remove(\"regras.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text contrações\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "changed_sentences = []\n",
    "for sentence in corpus.sentences.values():\n",
    "    if not sentence.text.endswith(sentence.tokens[-1].word):\n",
    "        contractions = [[int(x.id.split(\"-\")[0]), int(x.id.split(\"-\")[1]), x.word] for x in sentence.tokens if '-' in x.id]\n",
    "        new_text = [x.word for x in sentence.tokens if not '-' in x.id]\n",
    "        for contraction in reversed(contractions):\n",
    "            start = contraction[0]\n",
    "            end = contraction[1]\n",
    "            word = contraction[2]\n",
    "            for i in reversed(range(start-1, end)):\n",
    "                new_text.pop(i)\n",
    "            new_text.insert(start-1, word)\n",
    "        new_text = \" \".join(new_text)\n",
    "        sentence.metadados['text'] = new_text\n",
    "        changed_sentences.append(sentence.sent_id)\n",
    "\n",
    "print(\"|\".join(changed_sentences))\n",
    "\n",
    "corpus.save(\"regras.conllu\")\n",
    "if changed_sentences:\n",
    "    os.system(\"meld --diff {} regras.conllu\".format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminando pontuação duplicada\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "changed_sentences = []\n",
    "for sentid, sentence in corpus.sentences.items():\n",
    "    if sentence.tokens[-1].word == \".\" and sentence.tokens[-2].word == \".\":\n",
    "        sentence = tokenization.addToken(corpus, sentid, 'rm', sentence.tokens[-1].id).sentences[sentid]\n",
    "        sentence.tokens[-1].dephead = [x for x in sentence.tokens if x.deprel == \"root\"][0].id\n",
    "        changed_sentences.append(sentence.sent_id)\n",
    "    if sentence.tokens[-1].word == \"..\":\n",
    "        sentence.tokens[-1].word = \".\"\n",
    "        sentence.tokens[-1].lemma = \".\"\n",
    "        sentence.tokens[-1].dephead = [x for x in sentence.tokens if x.deprel == \"root\"][0].id\n",
    "        changed_sentences.append(sentence.sent_id)\n",
    "    if sentence.text.endswith(\"..\"):\n",
    "        sentence.metadados['text'] = sentence.text[:-1]\n",
    "        changed_sentences.append(sentence.sent_id)\n",
    "\n",
    "print(\"|\".join(changed_sentences))\n",
    "\n",
    "corpus.save(\"regras.conllu\")\n",
    "if changed_sentences:\n",
    "    os.system(\"meld --diff {} regras.conllu\".format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens com ..\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "for sentid, sentence in corpus.sentences.items():\n",
    "    if sentence.tokens[-1].word.endswith(\"..\") and (sentence.tokens[-1].upos == \"PUNCT\" or sentence.tokens[-1].deprel == \"punct\"):\n",
    "        sentence.tokens[-1].upos = \"EPA\"\n",
    "        sentence.tokens[-1].misc = sentence.tokens[-1].misc.replace(\"tokenização\", \"\").replace(\"||\", \"|\")\n",
    "        if not sentence.tokens[-1].misc:\n",
    "            sentence.tokens[-1].misc = \"_\"\n",
    "\n",
    "corpus.save(\"regras.conllu\")\n",
    "os.system(\"python3 ../../conllu-merge-resolver/cosmo.py {} regras.conllu '.*'\".format(file))\n",
    "os.system(\"cp {} antes-{}\".format(file, file_name))\n",
    "os.system(\"rm regras-correcao.zip; zip regras-correcao.zip antes-{} regras.conllu\".format(file_name))\n",
    "os.remove(\"antes-{}\".format(file_name))\n",
    "os.remove(\"regras.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens com .. parte 2\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "changed_sentences = []\n",
    "for sentid, sentence in corpus.sentences.items():\n",
    "    if sentence.tokens[-1].word.endswith(\"..\"):\n",
    "        sentence = tokenization.addToken(corpus, sentid, 'add', str(int(sentence.tokens[-1].id)+1), new_tokens=[]).sentences[sentid]\n",
    "        sentence.tokens[-1].dephead = [x for x in sentence.tokens if x.deprel == \"root\"][0].id\n",
    "        sentence.tokens[-1].lemma = \".\"\n",
    "        sentence.tokens[-1].word = \".\"\n",
    "        sentence.tokens[-1].deprel = \"punct\"\n",
    "        sentence.tokens[-1].upos = \"PUNCT\"\n",
    "        sentence.tokens[-2].word = sentence.tokens[-2].word[:-2]\n",
    "        sentence.tokens[-2].lemma = sentence.tokens[-2].lemma[:-2]\n",
    "        changed_sentences.append(sentence.sent_id)\n",
    "\n",
    "print(\"|\".join(changed_sentences))\n",
    "\n",
    "corpus.save(\"regras.conllu\")\n",
    "if changed_sentences:\n",
    "    os.system(\"meld --diff {} regras.conllu\".format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminando frases\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "old_sentences = set(corpus.sentences.keys())\n",
    "corpus.sentences = {x: corpus.sentences[x] for x in corpus.sentences if not x.startswith(\"258-\")}\n",
    "corpus.sentences = {x: corpus.sentences[x] for x in corpus.sentences if not any(y in corpus.sentences[x].to_str() for y in \"sentenciação\".split())}\n",
    "corpus.sentences = {x: corpus.sentences[x] for x in corpus.sentences if not any(y in corpus.sentences[x].to_str() for y in \"tokenização\".split())}\n",
    "corpus.sentences = {x: corpus.sentences[x] for x in corpus.sentences if not any(y in corpus.sentences[x].to_str() for y in \"eliminar\".split())}\n",
    "new_sentences = set(corpus.sentences.keys())\n",
    "\n",
    "if new_sentences != old_sentences:\n",
    "    print(len(old_sentences))\n",
    "    print(len(new_sentences))\n",
    "\n",
    "corpus.save(\"regras.conllu\")\n",
    "\n",
    "if new_sentences != old_sentences:\n",
    "    os.system(\"meld --diff {} regras.conllu\".format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATE.PY => erros de formato devem ser corrigidos\n",
    "os.system('~/Documents/Interrogat-rio/www/cgi-bin/tools/validate.py {} --lang=en'.format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER TO COMMIT BEFORE THIS\n",
    "# list sentences removed\n",
    "pre_commit = \"5f512e8986b79be994e2c89787086b670aa7089d\"\n",
    "pos_commit = \"master\"\n",
    "\n",
    "corpus = os.popen(\"git show {}:Petroles_3/{}\".format(pre_commit, file)).read()\n",
    "corpus_pre = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_pre.build(corpus)\n",
    "\n",
    "corpus = os.popen(\"git show {}:Petroles_3/{}\".format(pos_commit, file)).read()\n",
    "corpus_pos = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_pos.build(corpus)\n",
    "\n",
    "stats = count_all(\"pré\", corpus_pre)\n",
    "stats = count_all(\"pós\", corpus_pos, stats)\n",
    "\n",
    "sent_ids = set(corpus_pre.sentences)\n",
    "sent_ids_removed = sent_ids - set(corpus_pos.sentences)\n",
    "print(\"|\".join(sent_ids_removed))\n",
    "\n",
    "pprint.pprint(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remover sema, xpos, misc, nmod:appos (só para release)\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "experimento_obl_arg = \"experimento-obl-arg.conllu\" #False\n",
    "\n",
    "for sentid, sentence in corpus.sentences.items():\n",
    "    for token in sentence.tokens:\n",
    "        token.deps = \"_\"\n",
    "        token.xpos = \"_\"\n",
    "        token.misc = \"_\"\n",
    "        if token.deprel == \"nmod:appos\" and token.upos == \"NUM\":\n",
    "            token.deprel = \"nummod\"\n",
    "        elif token.deprel == \"nmod:appos\":\n",
    "            token.deprel = \"nmod\"\n",
    "        if token.deprel == \"expl:impers\":\n",
    "            token.deprel = \"expl\"\n",
    "\n",
    "corpus.save(release_file)\n",
    "\n",
    "s_obl_arg = 0\n",
    "t_obl_arg = 0\n",
    "if experimento_obl_arg:\n",
    "    for sentid, sentence in corpus.sentences.items():\n",
    "        if 'obl:arg' in sentence.to_str():\n",
    "            s_obl_arg += 1\n",
    "        for token in sentence.tokens:\n",
    "            if token.deprel == \"obl:arg\":\n",
    "                token.deprel = \"obl\"\n",
    "                t_obl_arg += 1\n",
    "    corpus.save(experimento_obl_arg)\n",
    "    print(\"Tokens com obl:arg convertidos: {}\\nFrases com obl:arg: {}\".format(t_obl_arg, s_obl_arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPARAR TRAIN E TEST\n",
    "copy_sentences_from = \"PetroGold-v1/petrogold\" # or False if random\n",
    "\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(release_file)\n",
    "\n",
    "if experimento_obl_arg:\n",
    "    obl_arg = estrutura_ud.Corpus(recursivo=True)\n",
    "    obl_arg.load(experimento_obl_arg)\n",
    "\n",
    "sentences = set(corpus.sentences)\n",
    "partitions = {}\n",
    "if not copy_sentences_from:\n",
    "    shuffled = list(corpus.sentences)\n",
    "    random.shuffle(shuffled)\n",
    "    partitions['test'] = set(shuffled[:round(0.05*len(sentences))])\n",
    "    partitions['dev'] = set([x for x in shuffled if x not in partitions['test']][:round(0.05*len(sentences))])\n",
    "    partitions['train'] = sentences - partitions['test'] - partitions['dev']\n",
    "else:\n",
    "    for kind in [\"train\", \"dev\", \"test\"]:\n",
    "        model = estrutura_ud.Corpus(recursivo=True)\n",
    "        model.load(\"{}-{}.conllu\".format(copy_sentences_from, kind))\n",
    "        partitions[kind] = set(model.sentences)\n",
    "\n",
    "assert not partitions['train'].intersection(partitions['test'])\n",
    "assert not partitions['train'].intersection(partitions['dev'])\n",
    "assert not partitions['test'].intersection(partitions['dev'])\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "dev = []\n",
    "for sentid, sentence in corpus.sentences.items():\n",
    "    if sentid in partitions['test']:\n",
    "        test.append(sentence.to_str())\n",
    "    if sentid in partitions['train']:\n",
    "        train.append(sentence.to_str())\n",
    "    if sentid in partitions['dev']:\n",
    "        dev.append(sentence.to_str())\n",
    "\n",
    "if experimento_obl_arg:\n",
    "    train_obl_arg = []\n",
    "    test_obl_arg = []\n",
    "    dev_obl_arg = []\n",
    "    for sentid, sentence in obl_arg.sentences.items():\n",
    "        if sentid in partitions['test']:\n",
    "            test_obl_arg.append(sentence.to_str())\n",
    "        if sentid in partitions['train']:\n",
    "            train_obl_arg.append(sentence.to_str())\n",
    "        if sentid in partitions['dev']:\n",
    "            dev_obl_arg.append(sentence.to_str())\n",
    "\n",
    "corpus_train = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_test = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_dev = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_train.build(\"\\n\\n\".join(train) + \"\\n\\n\")\n",
    "corpus_test.build(\"\\n\\n\".join(test) + \"\\n\\n\")\n",
    "corpus_dev.build(\"\\n\\n\".join(dev) + \"\\n\\n\")\n",
    "\n",
    "stats = count_all(\"train\", corpus_train)\n",
    "stats = count_all(\"test\", corpus_test, stats)\n",
    "stats = count_all(\"dev\", corpus_dev, stats)\n",
    "pprint.pprint(stats)\n",
    "\n",
    "corpus_train.save(\"petrogold-train.conllu\")\n",
    "corpus_test.save(\"petrogold-test.conllu\")\n",
    "corpus_dev.save(\"petrogold-dev.conllu\")\n",
    "\n",
    "if experimento_obl_arg:\n",
    "    print(\"Experimento obl:arg:\")\n",
    "    corpus_train = estrutura_ud.Corpus(recursivo=True)\n",
    "    corpus_test = estrutura_ud.Corpus(recursivo=True)\n",
    "    corpus_dev = estrutura_ud.Corpus(recursivo=True)\n",
    "    corpus_train.build(\"\\n\\n\".join(train_obl_arg) + \"\\n\\n\")\n",
    "    corpus_test.build(\"\\n\\n\".join(test_obl_arg) + \"\\n\\n\")\n",
    "    corpus_dev.build(\"\\n\\n\".join(dev_obl_arg) + \"\\n\\n\")\n",
    "\n",
    "    stats = count_all(\"train\", corpus_train)\n",
    "    stats = count_all(\"test\", corpus_test, stats)\n",
    "    stats = count_all(\"dev\", corpus_dev, stats)\n",
    "    pprint.pprint(stats)\n",
    "\n",
    "    corpus_train.save(\"experimento-obl-arg-train.conllu\")\n",
    "    corpus_test.save(\"experimento-obl-arg-test.conllu\")\n",
    "    corpus_dev.save(\"experimento-obl-arg-dev.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TREINAR COM TRAIN E DEV\n",
    "! ./udpipe-1.2.0 --train --tokenizer=none \"petrogold.udpipe\" \"petrogold-train.conllu\" \"petrogold-dev.conllu\"\n",
    "! ./udpipe-1.2.0 --train --tokenizer=none \"experimento-obl-arg.udpipe\" \"experimento-obl-arg-train.conllu\" \"experimento-obl-arg-dev.conllu\"\n",
    "#! ./udpipe-1.2.0 --train --tokenizer=none \"bosque-ud-2.8/bosque2.8.udpipe\" \"bosque-ud-2.8/pt_bosque-ud-train.conllu\" \"bosque-ud-2.8/pt_bosque-ud-dev.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE PETRO3\n",
    "! python3 ~/Documents/ACDC-UD/tokenizar_conllu.py \"petrogold-test.conllu\" \"petrogold-test-tokenizado.txt\"\n",
    "! python3 ~/Documents/ACDC-UD/udpipe_vertical.py \"petrogold.udpipe\" \"petrogold-test-tokenizado.txt\" \"petrogold-test-udpipe.conllu\"\n",
    "! python3 ~/Documents/ACDC-UD/conll18_ud_eval.py -v \"petrogold-test.conllu\" \"petrogold-test-udpipe.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE PETRO3 EXPERIMENTO OBL:ARG\n",
    "! python3 ~/Documents/ACDC-UD/tokenizar_conllu.py \"experimento-obl-arg-test.conllu\" \"experimento-obl-arg-test-tokenizado.txt\"\n",
    "! python3 ~/Documents/ACDC-UD/udpipe_vertical.py \"experimento-obl-arg.udpipe\" \"experimento-obl-arg-test-tokenizado.txt\" \"experimento-obl-arg-test-udpipe.conllu\"\n",
    "! python3 ~/Documents/ACDC-UD/conll18_ud_eval.py -v \"experimento-obl-arg-test.conllu\" \"experimento-obl-arg-test-udpipe.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE BOSQUE2.8\n",
    "! python3 ~/Documents/ACDC-UD/tokenizar_conllu.py \"bosque-ud-2.8/pt_bosque-ud-test.conllu\" \"bosque-ud-2.8/pt_bosque-ud-test-tokenizado.txt\"\n",
    "! python3 ~/Documents/ACDC-UD/udpipe_vertical.py \"bosque-ud-2.8/bosque2.8.udpipe\" \"bosque-ud-2.8/pt_bosque-ud-test-tokenizado.txt\" \"bosque-ud-2.8/pt_bosque-ud-test-udpipe.conllu\"\n",
    "! python3 ~/Documents/ACDC-UD/conll18_ud_eval.py -v \"bosque-ud-2.8/pt_bosque-ud-test.conllu\" \"bosque-ud-2.8/pt_bosque-ud-test-udpipe.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNT MODIFICATIONS (usado só no STIL)\n",
    "file = \"Petroles_3_golden.conllu\"\n",
    "file_stanza = \"Petroles_3_stanza.conllu\"\n",
    "file_udpipe = \"Petroles_3_udpipe.conllu\"\n",
    "post_changes = \"master\"\n",
    "\n",
    "corpus = os.popen(\"git show {}:Petroles_3/{}\".format(post_changes, file)).read()\n",
    "corpus_pos = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_pos.build(corpus)\n",
    "\n",
    "corpus_stanza = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_stanza.load(file_stanza)\n",
    "corpus_udpipe = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_udpipe.load(file_udpipe)\n",
    "\n",
    "sentences_that_matter = list(corpus_pos.sentences.keys())\n",
    "print(\"Sentences that matter: {}\".format(len(sentences_that_matter)))\n",
    "\n",
    "tokens_changed = set()\n",
    "tokens_changed_lemma = set()\n",
    "tokens_changed_deprel = set()\n",
    "tokens_changed_dephead = set()\n",
    "tokens_changed_upos = set()\n",
    "sentences_changed = set()\n",
    "udpipe_correct = set()\n",
    "all_tokens = 0\n",
    "all_sentences = 0\n",
    "cm = set()\n",
    "for sentid in sentences_that_matter:\n",
    "    for t, token in enumerate(corpus_pos.sentences[sentid].tokens):\n",
    "        if not '-' in token.id and token.upos != \"PUNCT\":\n",
    "            all_tokens += 1\n",
    "            try:\n",
    "                token_pos = corpus_pos.sentences[sentid].tokens[t]\n",
    "                token_stanza = corpus_stanza.sentences[sentid].tokens[t]\n",
    "                token_udpipe = corpus_udpipe.sentences[sentid].tokens[t]\n",
    "                important_cols_pos = [token_pos.lemma, token_pos.upos, token_pos.feats, token_pos.dephead, token_pos.deprel]\n",
    "                important_cols_stanza = [token_stanza.lemma, token_stanza.upos, token_stanza.feats, token_stanza.dephead, token_stanza.deprel]\n",
    "                important_cols_udpipe = [token_udpipe.lemma, token_udpipe.upos, token_udpipe.feats, token_udpipe.dephead, token_udpipe.deprel]\n",
    "                if token_stanza.deprel != token_udpipe.deprel:\n",
    "                    cm.add(\"{}-{}\".format(sentid, t))\n",
    "                    if token_udpipe.deprel == token_pos.deprel:\n",
    "                        udpipe_correct.add(\"{}-{}\".format(sentid, t))\n",
    "                if important_cols_pos != important_cols_stanza:\n",
    "                    #if token_pos.deprel == \"obl:arg\" and token_stanza.deprel in [\"obj\", \"iobj\"]:\n",
    "                        #continue\n",
    "                    tokens_changed.add(\"{}-{}\".format(sentid, t))\n",
    "                    if not sentid in sentences_changed:\n",
    "                        sentences_changed.add(sentid)\n",
    "                    if token_pos.lemma != token_stanza.lemma:\n",
    "                        tokens_changed_lemma.add(\"{}-{}\".format(sentid, t))\n",
    "                    if token_pos.upos != token_stanza.upos:\n",
    "                        tokens_changed_upos.add(\"{}-{}\".format(sentid, t))\n",
    "                    if token_pos.dephead != token_stanza.dephead:\n",
    "                        tokens_changed_dephead.add(\"{}-{}\".format(sentid, t))\n",
    "                    if token_pos.deprel != token_stanza.deprel:\n",
    "                        tokens_changed_deprel.add(\"{}-{}\".format(sentid, t))\n",
    "            except:\n",
    "                print(\"token out of index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAY INTERSEÇÕES (usado só no STIL)\n",
    "print(\"EXCEPT PUNCT -- Tokens changed: {} / All_tokens: {}\".format(len(tokens_changed), all_tokens))\n",
    "print(\"Sentences changed: {} / All_sentences: {}\".format(len(sentences_changed), len(sentences_that_matter)))\n",
    "print(\"CM: {} / Intersection: {} / UDPIPE_CORRECT: {}\".format(len(cm), len(tokens_changed.intersection(cm)), len(udpipe_correct)))\n",
    "print(len(tokens_changed_lemma))\n",
    "print(len(tokens_changed_upos))\n",
    "print(len(tokens_changed_dephead))\n",
    "print(len(tokens_changed_deprel))\n",
    "print(\"\")\n",
    "print(len(tokens_changed_lemma.intersection(cm)))\n",
    "print(len(tokens_changed_upos.intersection(cm)))\n",
    "print(len(tokens_changed_dephead.intersection(cm)))\n",
    "print(len(tokens_changed_deprel.intersection(cm)))\n",
    "print(\"\")\n",
    "print(len(tokens_changed_lemma.intersection(tokens_changed_upos)))\n",
    "print(len(tokens_changed_lemma.intersection(tokens_changed_upos).intersection(cm)))\n",
    "print(len(tokens_changed_lemma.intersection(tokens_changed_deprel)))\n",
    "print(len(tokens_changed_lemma.intersection(tokens_changed_deprel).intersection(cm)))\n",
    "print(len(tokens_changed_upos.intersection(tokens_changed_deprel)))\n",
    "print(len(tokens_changed_upos.intersection(tokens_changed_deprel).intersection(cm)))\n",
    "print(len(tokens_changed_dephead.intersection(tokens_changed_deprel)))\n",
    "print(len(tokens_changed_dephead.intersection(tokens_changed_deprel).intersection(cm)))\n",
    "print(len(tokens_changed_lemma - tokens_changed_upos - tokens_changed_dephead - tokens_changed_deprel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
