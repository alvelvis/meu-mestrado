{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isfile(\"estrutura_ud.py\"):\n",
    "    os.system(\"wget https://raw.githubusercontent.com/alvelvis/ACDC-UD/master/estrutura_ud.py\")\n",
    "if not os.path.isfile(\"tokenization.py\"):\n",
    "    os.system(\"wget https://raw.githubusercontent.com/alvelvis/ACDC-UD/master/tokenization.py\")\n",
    "import estrutura_ud\n",
    "import tokenization\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "file = \"Petroles_3_golden.conllu\"\n",
    "release_file = \"PetroGold.conllu\"\n",
    "\n",
    "def change_col(col, token, value):\n",
    "    col = token.__dict__[col].split(\"|\")\n",
    "    col = [x for x in col if not x.startswith(value.split(\"=\")[0]) and x != \"_\"] + [value]\n",
    "    col = \"|\".join(sorted(col))\n",
    "    return col\n",
    "\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "def count_tokens(corpus, i=0):\n",
    "    for sentence in corpus.sentences.values():\n",
    "        i += len([x for x in sentence.tokens if not '-' in x.id])\n",
    "    return i\n",
    "\n",
    "def count_words(corpus, i=0):\n",
    "    for sentence in corpus.sentences.values():\n",
    "        i += len([x for x in sentence.tokens if not '-' in x.id and x.upos != \"PUNCT\"])\n",
    "    return i\n",
    "\n",
    "def count_documents(corpus):\n",
    "    return len(set([x.rsplit(\"-\", 1)[0] for x in corpus.sentences]))\n",
    "\n",
    "def count_entities(corpus, i=0):\n",
    "    for sentence in corpus.sentences.values():\n",
    "        i += len([x for x in sentence.tokens if 'B=' in x.deps])\n",
    "    return i\n",
    "\n",
    "def count_all(label, corpus, stats=\"\"):\n",
    "    if not stats:\n",
    "        stats = {'sentences': {}, 'tokens': {}, 'entities': {}, 'documents': {}, 'words': {}}\n",
    "    stats['sentences'][label] = len(corpus.sentences)\n",
    "    stats['tokens'][label] = count_tokens(corpus)\n",
    "    stats['documents'][label] = count_documents(corpus)\n",
    "    stats['entities'][label] = count_entities(corpus)\n",
    "    stats['words'][label] = count_words(corpus)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COSMO sem root e ciclo\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "for sentence in corpus.sentences.values():\n",
    "    if not any(x.deprel == \"root\" for x in sentence.tokens):\n",
    "        sentence.tokens[0].misc = \"SEM ROOT\"\n",
    "    elif sentence.to_str().count(\"\\troot\\t\") > 1:\n",
    "        for token in sentence.tokens:\n",
    "            if token.deprel == \"root\":\n",
    "                token.misc = \"MAIS DE 1 ROOT\"\n",
    "    else:\n",
    "        for token in sentence.tokens:\n",
    "            if (\n",
    "                token.dephead != \"0\" and (\n",
    "                    token.dephead == token.head_token.dephead or \n",
    "                    token.dephead == token.id or \n",
    "                    token.id == token.head_token.dephead or \n",
    "                    (token.head_token.dephead != \"0\" and token.head_token.head_token.dephead == token.id) or\n",
    "                    (token.head_token.dephead != \"0\" and token.head_token.head_token.dephead != \"0\" and token.head_token.head_token.head_token.dephead == token.id)\n",
    "                )):\n",
    "                token.misc = \"CICLO\"\n",
    "            if token.dephead == \"0\" and token.deprel != \"root\":\n",
    "                token.misc = \"root\"\n",
    "            if token.dephead != \"0\" and token.deprel == \"root\":\n",
    "                token.misc = \"root\"\n",
    "\n",
    "file_name = file.rsplit(\"/\", 1)[1] if '/' in file else file\n",
    "corpus.save(\"regras.conllu\")\n",
    "os.system(\"python3 ../../conllu-merge-resolver/cosmo.py {} regras.conllu '.*'\".format(file))\n",
    "os.system(\"cp {} antes-{}\".format(file, file_name))\n",
    "os.system(\"rm regras-correcao.zip; zip regras-correcao.zip antes-{} regras.conllu\".format(file_name))\n",
    "os.remove(\"antes-{}\".format(file_name))\n",
    "os.remove(\"regras.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correções de formato: ordem alfabética das feats, =, e deprels que não podem ter filho\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "for sentence in corpus.sentences.values():\n",
    "    for token in sentence.tokens:\n",
    "        if '</' in token.to_str():\n",
    "            token.misc = \"</\"\n",
    "        if ' ' in token.to_str():\n",
    "            token.misc = \"espaço em branco\"\n",
    "        if not token.feats.strip():\n",
    "            token.feats = \"_\"\n",
    "        if token.feats != \"_\":\n",
    "            try:\n",
    "                parts = [x.split('=') for x in token.feats.split(\"|\")]\n",
    "                parts = dict(parts)\n",
    "            except ValueError:\n",
    "                token.misc = \"feats com erro\"\n",
    "            token.feats = \"|\".join(sorted(token.feats.split(\"|\"), key=lambda x: x.lower()))\n",
    "    \n",
    "    cant_have_children = \"flat:name compound punct fixed aux cop case mark punct\".split()\n",
    "    for token in sentence.tokens:\n",
    "        if token.deprel not in \"fixed compound\".split() and token.head_token.deprel in cant_have_children:\n",
    "            token.dephead = token.head_token.dephead if token.head_token.head_token.deprel not in cant_have_children else token.head_token.head_token.dephead\n",
    "\n",
    "file_name = file.rsplit(\"/\", 1)[1] if '/' in file else file\n",
    "corpus.save(\"regras.conllu\")\n",
    "os.system(\"python3 ../../conllu-merge-resolver/cosmo.py {} regras.conllu '.*'\".format(file))\n",
    "os.system(\"cp {} antes-{}\".format(file, file_name))\n",
    "os.system(\"rm regras-correcao.zip; zip regras-correcao.zip antes-{} regras.conllu\".format(file_name))\n",
    "os.remove(\"antes-{}\".format(file_name))\n",
    "os.remove(\"regras.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT IGUAIS\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "texts = {}\n",
    "for sentid, sentence in corpus.sentences.items():\n",
    "    text = sentence.text\n",
    "    if not text in texts:\n",
    "        texts[text] = []\n",
    "    texts[text].append(sentid)\n",
    "\n",
    "with open(\"frases-repetidas.txt\", \"w+\") as f:\n",
    "    for text in texts:\n",
    "        if len(texts[text]) >= 2:\n",
    "            f.write(\"{}\\n{}\\n\\n\".format(text, \"|\".join(texts[text])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text contrações\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "changed_sentences = []\n",
    "for sentence in corpus.sentences.values():\n",
    "    if not sentence.text.endswith(sentence.tokens[-1].word):\n",
    "        contractions = [[int(x.id.split(\"-\")[0]), int(x.id.split(\"-\")[1]), x.word] for x in sentence.tokens if '-' in x.id]\n",
    "        new_text = [x.word for x in sentence.tokens if not '-' in x.id]\n",
    "        for contraction in reversed(contractions):\n",
    "            start = contraction[0]\n",
    "            end = contraction[1]\n",
    "            word = contraction[2]\n",
    "            for i in reversed(range(start-1, end)):\n",
    "                new_text.pop(i)\n",
    "            new_text.insert(start-1, word)\n",
    "        new_text = \" \".join(new_text)\n",
    "        sentence.metadados['text'] = new_text\n",
    "        changed_sentences.append(sentence.sent_id)\n",
    "\n",
    "print(\"|\".join(changed_sentences))\n",
    "\n",
    "corpus.save(\"regras.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminando pontuação duplicada\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "changed_sentences = []\n",
    "for sentid, sentence in corpus.sentences.items():\n",
    "    if sentence.tokens[-1].word == \".\" and sentence.tokens[-2].word == \".\":\n",
    "        sentence = tokenization.addToken(corpus, sentid, 'rm', sentence.tokens[-1].id).sentences[sentid]\n",
    "        sentence.tokens[-1].dephead = [x for x in sentence.tokens if x.deprel == \"root\"][0].id\n",
    "        changed_sentences.append(sentence.sent_id)\n",
    "    if sentence.tokens[-1].word == \"..\":\n",
    "        sentence.tokens[-1].word = \".\"\n",
    "        sentence.tokens[-1].lemma = \".\"\n",
    "        sentence.tokens[-1].dephead = [x for x in sentence.tokens if x.deprel == \"root\"][0].id\n",
    "        changed_sentences.append(sentence.sent_id)\n",
    "    if sentence.text.endswith(\"..\"):\n",
    "        sentence.metadados['text'] = sentence.text[:-1]\n",
    "        changed_sentences.append(sentence.sent_id)\n",
    "\n",
    "print(\"|\".join(changed_sentences))\n",
    "\n",
    "corpus.save(\"regras.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens com ..\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "for sentid, sentence in corpus.sentences.items():\n",
    "    if sentence.tokens[-1].word.endswith(\"..\") and (sentence.tokens[-1].upos == \"PUNCT\" or sentence.tokens[-1].deprel == \"punct\"):\n",
    "        sentence.tokens[-1].upos = \"EPA\"\n",
    "        sentence.tokens[-1].misc = sentence.tokens[-1].misc.replace(\"tokenização\", \"\").replace(\"||\", \"|\")\n",
    "        if not sentence.tokens[-1].misc:\n",
    "            sentence.tokens[-1].misc = \"_\"\n",
    "\n",
    "corpus.save(\"regras.conllu\")\n",
    "os.system(\"python3 ../../conllu-merge-resolver/cosmo.py {} regras.conllu '.*'\".format(file))\n",
    "os.system(\"cp {} antes-{}\".format(file, file_name))\n",
    "os.system(\"rm regras-correcao.zip; zip regras-correcao.zip antes-{} regras.conllu\".format(file_name))\n",
    "os.remove(\"antes-{}\".format(file_name))\n",
    "os.remove(\"regras.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens com .. parte 2\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "changed_sentences = []\n",
    "for sentid, sentence in corpus.sentences.items():\n",
    "    if sentence.tokens[-1].word.endswith(\"..\"):\n",
    "        sentence = tokenization.addToken(corpus, sentid, 'add', str(int(sentence.tokens[-1].id)+1), new_tokens=[]).sentences[sentid]\n",
    "        sentence.tokens[-1].dephead = [x for x in sentence.tokens if x.deprel == \"root\"][0].id\n",
    "        sentence.tokens[-1].lemma = \".\"\n",
    "        sentence.tokens[-1].word = \".\"\n",
    "        sentence.tokens[-1].deprel = \"punct\"\n",
    "        sentence.tokens[-1].upos = \"PUNCT\"\n",
    "        sentence.tokens[-2].word = sentence.tokens[-2].word[:-2]\n",
    "        sentence.tokens[-2].lemma = sentence.tokens[-2].lemma[:-2]\n",
    "        changed_sentences.append(sentence.sent_id)\n",
    "\n",
    "print(\"|\".join(changed_sentences))\n",
    "\n",
    "corpus.save(\"regras.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminando frases\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "old_sentences = set(corpus.sentences.keys())\n",
    "corpus.sentences = {x: corpus.sentences[x] for x in corpus.sentences if not x.startswith(\"258-\")}\n",
    "corpus.sentences = {x: corpus.sentences[x] for x in corpus.sentences if not any(y in corpus.sentences[x].to_str() for y in \"sentenciação\".split())}\n",
    "corpus.sentences = {x: corpus.sentences[x] for x in corpus.sentences if not any(y in corpus.sentences[x].to_str() for y in \"tokenização\".split())}\n",
    "corpus.sentences = {x: corpus.sentences[x] for x in corpus.sentences if not any(y in corpus.sentences[x].to_str() for y in \"eliminar\".split())}\n",
    "new_sentences = set(corpus.sentences.keys())\n",
    "\n",
    "if new_sentences != old_sentences:\n",
    "    print(len(new_sentences))\n",
    "    print(len(old_sentences))\n",
    "\n",
    "corpus.save(\"regras.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: Your local changes to the following files would be overwritten by checkout:\n",
      "\tPetroles_3/prepare_release.ipynb\n",
      "Please commit your changes or stash them before you switch branches.\n",
      "Aborting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49118/2963893186.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"git checkout {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_commit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcorpus_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestrutura_ud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecursivo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcorpus_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"git checkout {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_commit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/meu-mestrado/Petroles_3/estrutura_ud.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    202\u001b[0m                                                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                                                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many_of_keywords\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many_of_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                                                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                                                         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many_of_keywords\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"# sent_id = \"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                                                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_not_built\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# sent_id = \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/meu-mestrado/Petroles_3/estrutura_ud.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, txt)\u001b[0m\n\u001b[1;32m    171\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                                 \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecursivo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecursivo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                                 \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/meu-mestrado/Petroles_3/estrutura_ud.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, txt, sent_id)\u001b[0m\n\u001b[1;32m    119\u001b[0m                                 \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                                         \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"string\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"color\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"head_token\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"next_token\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"previous_token\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"separator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"children\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"<tok>\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecursivo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                                         \u001b[0mtoken_dephead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdephead\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m'<'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdephead\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"<.*?>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdephead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# list sentences removed\n",
    "pre_commit = \"48efd0a9d2092cd8f4dd17c0eda7bcc85522f02c\"\n",
    "pos_commit = \"master\"\n",
    "\n",
    "os.system(\"git checkout {}\".format(pre_commit))\n",
    "corpus_pre = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_pre.load(file)\n",
    "\n",
    "os.system(\"git checkout {}\".format(pos_commit))\n",
    "corpus_pos = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_pos.load(file)\n",
    "\n",
    "stats = count_all(\"pré\", corpus_pre)\n",
    "stats = count_all(\"pós\", corpus_pos, stats)\n",
    "\n",
    "sent_ids = set(corpus_pre.sentences)\n",
    "sent_ids_removed = sent_ids - set(corpus_pos.sentences)\n",
    "print(\"|\".join(sent_ids_removed))\n",
    "\n",
    "pprint.pprint(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 8.406656980514526"
     ]
    }
   ],
   "source": [
    "# remover sema, xpos, misc, nmod:appos (só para release)\n",
    "corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus.load(file)\n",
    "\n",
    "for sentid, sentence in corpus.sentences.items():\n",
    "    for token in sentence.tokens:\n",
    "        token.deps = \"_\"\n",
    "        token.xpos = \"_\"\n",
    "        token.misc = \"_\"\n",
    "        if token.deprel == \"nmod:appos\":\n",
    "            token.deprel = \"nmod\"\n",
    "        if token.deprel == \"expl:impers\":\n",
    "            token.deprel = \"expl\"\n",
    "\n",
    "corpus.save(release_file)\n",
    "\n",
    "for sentid, sentence in corpus.sentences.items():\n",
    "    for token in sentence.tokens:\n",
    "        if token.deprel == \"obl:arg\":\n",
    "            token.deprel = \"obl\"\n",
    "\n",
    "corpus.save(\"experimento-obl-arg.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATE.PY => erros de formato devem ser corrigidos\n",
    "! ~/Documentos/Interrogat-rio/www/cgi-bin/tools/validate.py PetroGold.conllu --lang=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 0.5454018115997314build: 8.117834091186523build: 0.5099120140075684build: 9.190043687820435build: 7.901104927062988build: 8.214680671691895build: 8.56426191329956"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'documents': {'dev': 19, 'test': 19, 'train': 19},\n",
      " 'entities': {'dev': 0, 'test': 0, 'train': 0},\n",
      " 'sentences': {'dev': 447, 'test': 447, 'train': 8061},\n",
      " 'tokens': {'dev': 12763, 'test': 12214, 'train': 225857},\n",
      " 'words': {'dev': 11313, 'test': 10783, 'train': 199319}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 9.615033626556396build: 8.165860652923584build: 8.307520866394043build: 8.608531475067139"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'documents': {'dev': 19, 'test': 19, 'train': 19},\n",
      " 'entities': {'dev': 0, 'test': 0, 'train': 0},\n",
      " 'sentences': {'dev': 447, 'test': 447, 'train': 8061},\n",
      " 'tokens': {'dev': 12763, 'test': 12214, 'train': 225857},\n",
      " 'words': {'dev': 11313, 'test': 10783, 'train': 199319}}\n"
     ]
    }
   ],
   "source": [
    "# SEPARAR TRAIN E TEST\n",
    "files = [release_file, \"experimento-obl-arg.conllu\"]\n",
    "\n",
    "sentences = {}\n",
    "get_sentences_for_partitions = {}\n",
    "\n",
    "# comment this line if randomly generate partitions\n",
    "get_sentences_for_partitions = {\n",
    "    \"dev\": \"PetroGold-v1/petrogold-dev.conllu\",\n",
    "    \"train\": \"PetroGold-v1/petrogold-train.conllu\",\n",
    "    \"test\": \"PetroGold-v1/petrogold-test.conllu\"\n",
    "    }\n",
    "\n",
    "if get_sentences_for_partitions:\n",
    "    for partition in get_sentences_for_partitions:\n",
    "        corpus = estrutura_ud.Corpus()\n",
    "        corpus.load(get_sentences_for_partitions[partition])\n",
    "        sentences[partition] = set(corpus.sentences)        \n",
    "\n",
    "for file in files:\n",
    "    corpus = estrutura_ud.Corpus(recursivo=True)\n",
    "    corpus.load(file)\n",
    "\n",
    "    if not sentences:\n",
    "        all_sentences = set(corpus.sentences)\n",
    "        shuffled = list(corpus.sentences)\n",
    "        random.shuffle(shuffled)\n",
    "        sentences[\"test\"] = set(shuffled[:round(0.05*len(all_sentences))])\n",
    "        sentences[\"dev\"] = set([x for x in shuffled if x not in sentences['test']][:round(0.05*len(all_sentences))])\n",
    "        sentences[\"train\"] = all_sentences - sentences[\"test\"] - sentences[\"dev\"]\n",
    "    assert not sentences[\"train\"].intersection(sentences[\"test\"])\n",
    "    assert not sentences[\"train\"].intersection(sentences[\"dev\"])\n",
    "    assert not sentences[\"test\"].intersection(sentences[\"dev\"])\n",
    "\n",
    "    train = []\n",
    "    test = []\n",
    "    dev = []\n",
    "    for sentid, sentence in corpus.sentences.items():\n",
    "        if sentid in sentences[\"test\"]:\n",
    "            test.append(sentence.to_str())\n",
    "        if sentid in sentences[\"train\"]:\n",
    "            train.append(sentence.to_str())\n",
    "        if sentid in sentences[\"dev\"]:\n",
    "            dev.append(sentence.to_str())\n",
    "\n",
    "    corpus_train = estrutura_ud.Corpus(recursivo=True)\n",
    "    corpus_test = estrutura_ud.Corpus(recursivo=True)\n",
    "    corpus_dev = estrutura_ud.Corpus(recursivo=True)\n",
    "    corpus_train.build(\"\\n\\n\".join(train) + \"\\n\\n\")\n",
    "    corpus_test.build(\"\\n\\n\".join(test) + \"\\n\\n\")\n",
    "    corpus_dev.build(\"\\n\\n\".join(dev) + \"\\n\\n\")\n",
    "\n",
    "    stats = count_all(\"train\", corpus_train)\n",
    "    stats = count_all(\"test\", corpus_test, stats)\n",
    "    stats = count_all(\"dev\", corpus_dev, stats)\n",
    "    pprint.pprint(stats)\n",
    "\n",
    "    corpus_train.save(\"{}-train.conllu\".format(file.rsplit(\".\", 1)[0]))\n",
    "    corpus_test.save(\"{}-test.conllu\".format(file.rsplit(\".\", 1)[0]))\n",
    "    corpus_dev.save(\"{}-dev.conllu\".format(file.rsplit(\".\", 1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data: done.\n",
      "Training the UDPipe model.\n",
      "Tagger model 1 columns: lemma use=1/provide=1, xpostag use=1/provide=1, feats use=1/provide=1\n",
      "Creating morphological dictionary for tagger model 1.\n",
      "Tagger model 1 dictionary options: max_form_analyses=0, custom dictionary_file=none\n",
      "Tagger model 1 guesser options: suffix_rules=8, prefixes_max=4, prefix_min_count=10, enrich_dictionary=6\n",
      "Tagger model 1 options: iterations=20, early_stopping=0, templates=tagger\n",
      "Training tagger model 1.\n",
      "Iteration 1: done, accuracy 92.46%\n",
      "Iteration 2: done, accuracy 96.35%\n",
      "Iteration 3: done, accuracy 97.26%\n",
      "Iteration 4: done, accuracy 97.82%\n",
      "Iteration 5: done, accuracy 98.14%\n",
      "Iteration 6: done, accuracy 98.40%\n",
      "Iteration 7: done, accuracy 98.56%\n",
      "Iteration 8: done, accuracy 98.76%\n",
      "Iteration 9: done, accuracy 98.87%\n",
      "Iteration 10: done, accuracy 98.96%\n",
      "Iteration 11: done, accuracy 99.05%\n",
      "Iteration 12: done, accuracy 99.11%\n",
      "Iteration 13: done, accuracy 99.17%\n",
      "Iteration 14: done, accuracy 99.24%\n",
      "Iteration 15: done, accuracy 99.27%\n",
      "Iteration 16: done, accuracy 99.33%\n",
      "Iteration 17: done, accuracy 99.38%\n",
      "Iteration 18: done, accuracy 99.39%\n",
      "Iteration 19: done, accuracy 99.42%\n",
      "Iteration 20: done, accuracy 99.44%\n",
      "Parser transition options: system=projective, oracle=dynamic, structured_interval=8, single_root=1\n",
      "Parser uses lemmas/upos/xpos/feats: automatically generated by tagger\n",
      "Parser embeddings options: upostag=20, feats=20, xpostag=0, form=50, lemma=0, deprel=20\n",
      "  form mincount=2, precomputed form embeddings=none\n",
      "  lemma mincount=2, precomputed lemma embeddings=none\n",
      "Parser network options: iterations=10, hidden_layer=200, batch_size=10,\n",
      "  learning_rate=0.0200, learning_rate_final=0.0010, l2=0.5000, early_stopping=0\n",
      "Initialized 'universal_tag' embedding with 0,17 words and 0.0%,100.0% coverage.\n",
      "Initialized 'feats' embedding with 0,137 words and 0.0%,100.0% coverage.\n",
      "Initialized 'form' embedding with 0,9126 words and 0.0%,97.4% coverage.\n",
      "Initialized 'deprel' embedding with 0,39 words and 0.0%,100.0% coverage.\n",
      "Iteration 1: training logprob -2.5427e+05\n",
      "Iteration 2: training logprob -4.0748e+05\n",
      "Iteration 3: training logprob -3.0732e+05\n",
      "Iteration 4: training logprob -2.5235e+05\n",
      "Iteration 5: training logprob -2.1205e+05\n",
      "Iteration 6: training logprob -1.8045e+05\n",
      "Iteration 7: training logprob -1.6145e+05\n",
      "Iteration 8: training logprob -1.4975e+05\n",
      "Iteration 9: training logprob -1.3985e+05\n",
      "Iteration 10: training logprob -1.3183e+05\n",
      "The trained UDPipe model was saved.\n",
      "Loading training data: done.\n",
      "Training the UDPipe model.\n",
      "Tagger model 1 columns: lemma use=1/provide=1, xpostag use=1/provide=1, feats use=1/provide=1\n",
      "Creating morphological dictionary for tagger model 1.\n",
      "Tagger model 1 dictionary options: max_form_analyses=0, custom dictionary_file=none\n",
      "Tagger model 1 guesser options: suffix_rules=8, prefixes_max=4, prefix_min_count=10, enrich_dictionary=6\n",
      "Tagger model 1 options: iterations=20, early_stopping=0, templates=tagger\n",
      "Training tagger model 1.\n",
      "Iteration 1: done, accuracy 92.46%\n",
      "Iteration 2: done, accuracy 96.35%\n",
      "Iteration 3: done, accuracy 97.26%\n",
      "Iteration 4: done, accuracy 97.82%\n",
      "Iteration 5: done, accuracy 98.14%\n",
      "Iteration 6: done, accuracy 98.40%\n",
      "Iteration 7: done, accuracy 98.56%\n",
      "Iteration 8: done, accuracy 98.76%\n",
      "Iteration 9: done, accuracy 98.87%\n",
      "Iteration 10: done, accuracy 98.96%\n",
      "Iteration 11: done, accuracy 99.05%\n",
      "Iteration 12: done, accuracy 99.11%\n",
      "Iteration 13: done, accuracy 99.17%\n",
      "Iteration 14: done, accuracy 99.24%\n",
      "Iteration 15: done, accuracy 99.27%\n",
      "Iteration 16: done, accuracy 99.33%\n",
      "Iteration 17: done, accuracy 99.38%\n",
      "Iteration 18: done, accuracy 99.39%\n",
      "Iteration 19: done, accuracy 99.42%\n",
      "Iteration 20: done, accuracy 99.44%\n",
      "Parser transition options: system=projective, oracle=dynamic, structured_interval=8, single_root=1\n",
      "Parser uses lemmas/upos/xpos/feats: automatically generated by tagger\n",
      "Parser embeddings options: upostag=20, feats=20, xpostag=0, form=50, lemma=0, deprel=20\n",
      "  form mincount=2, precomputed form embeddings=none\n",
      "  lemma mincount=2, precomputed lemma embeddings=none\n",
      "Parser network options: iterations=10, hidden_layer=200, batch_size=10,\n",
      "  learning_rate=0.0200, learning_rate_final=0.0010, l2=0.5000, early_stopping=0\n",
      "Initialized 'universal_tag' embedding with 0,17 words and 0.0%,100.0% coverage.\n",
      "Initialized 'feats' embedding with 0,137 words and 0.0%,100.0% coverage.\n",
      "Initialized 'form' embedding with 0,9126 words and 0.0%,97.4% coverage.\n",
      "Initialized 'deprel' embedding with 0,38 words and 0.0%,100.0% coverage.\n",
      "Iteration 1: training logprob -2.4607e+05\n",
      "Iteration 2: training logprob -4.0687e+05\n",
      "Iteration 3: training logprob -3.0312e+05\n",
      "Iteration 4: training logprob -2.4648e+05\n",
      "Iteration 5: training logprob -2.0558e+05\n",
      "Iteration 6: training logprob -1.7568e+05\n",
      "Iteration 7: training logprob -1.5763e+05\n",
      "Iteration 8: training logprob -1.4631e+05\n",
      "Iteration 9: training logprob -1.3749e+05\n",
      "Iteration 10: training logprob -1.3035e+05\n",
      "The trained UDPipe model was saved.\n"
     ]
    }
   ],
   "source": [
    "# TREINAR COM TRAIN E DEV\n",
    "! ./udpipe-1.2.0 --train --tokenizer=none \"PetroGold.udpipe\" \"PetroGold-train.conllu\" \"PetroGold-dev.conllu\"\n",
    "! ./udpipe-1.2.0 --train --tokenizer=none \"experimento-obl-arg.udpipe\" \"experimento-obl-arg-train.conllu\" \"experimento-obl-arg-dev.conllu\"\n",
    "#! ./udpipe-1.2.0 --train --tokenizer=none \"bosque-ud-2.8/bosque2.8.udpipe\" \"bosque-ud-2.8/pt_bosque-ud-train.conllu\" \"bosque-ud-2.8/pt_bosque-ud-dev.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n",
      "Loading UDPipe model: done.\n",
      "Metric     | Precision |    Recall |  F1 Score | AligndAcc\n",
      "-----------+-----------+-----------+-----------+-----------\n",
      "Tokens     |    100.00 |    100.00 |    100.00 |\n",
      "Sentences  |    100.00 |    100.00 |    100.00 |\n",
      "Words      |    100.00 |    100.00 |    100.00 |\n",
      "UPOS       |     98.33 |     98.33 |     98.33 |     98.33\n",
      "XPOS       |    100.00 |    100.00 |    100.00 |    100.00\n",
      "UFeats     |     97.72 |     97.72 |     97.72 |     97.72\n",
      "AllTags    |     97.00 |     97.00 |     97.00 |     97.00\n",
      "Lemmas     |     98.53 |     98.53 |     98.53 |     98.53\n",
      "UAS        |     90.44 |     90.44 |     90.44 |     90.44\n",
      "LAS        |     88.82 |     88.82 |     88.82 |     88.82\n",
      "CLAS       |     83.57 |     83.23 |     83.40 |     83.23\n",
      "MLAS       |     80.38 |     80.05 |     80.22 |     80.05\n",
      "BLEX       |     81.94 |     81.60 |     81.77 |     81.60\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE PETRO3\n",
    "! python3 ~/Documentos/ACDC-UD/tokenizar_conllu.py \"PetroGold-test.conllu\" \"PetroGold-test-tokenizado.txt\"\n",
    "! python3 ~/Documentos/ACDC-UD/udpipe_vertical.py \"PetroGold.udpipe\" \"PetroGold-test-tokenizado.txt\" \"PetroGold-test-udpipe.conllu\"\n",
    "! python3 ~/Documentos/ACDC-UD/conll18_ud_eval.py -v \"PetroGold-test.conllu\" \"PetroGold-test-udpipe.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n",
      "Loading UDPipe model: done.\n",
      "Metric     | Precision |    Recall |  F1 Score | AligndAcc\n",
      "-----------+-----------+-----------+-----------+-----------\n",
      "Tokens     |    100.00 |    100.00 |    100.00 |\n",
      "Sentences  |    100.00 |    100.00 |    100.00 |\n",
      "Words      |    100.00 |    100.00 |    100.00 |\n",
      "UPOS       |     98.33 |     98.33 |     98.33 |     98.33\n",
      "XPOS       |    100.00 |    100.00 |    100.00 |    100.00\n",
      "UFeats     |     97.72 |     97.72 |     97.72 |     97.72\n",
      "AllTags    |     97.00 |     97.00 |     97.00 |     97.00\n",
      "Lemmas     |     98.53 |     98.53 |     98.53 |     98.53\n",
      "UAS        |     90.11 |     90.11 |     90.11 |     90.11\n",
      "LAS        |     88.32 |     88.32 |     88.32 |     88.32\n",
      "CLAS       |     82.69 |     82.31 |     82.50 |     82.31\n",
      "MLAS       |     79.42 |     79.05 |     79.24 |     79.05\n",
      "BLEX       |     81.02 |     80.65 |     80.84 |     80.65\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE EXPERIMENTO OBL ARG\n",
    "! python3 ~/Documentos/ACDC-UD/tokenizar_conllu.py \"experimento-obl-arg-test.conllu\" \"experimento-obl-arg-test-tokenizado.txt\"\n",
    "! python3 ~/Documentos/ACDC-UD/udpipe_vertical.py \"experimento-obl-arg.udpipe\" \"experimento-obl-arg-test-tokenizado.txt\" \"experimento-obl-arg-test-udpipe.conllu\"\n",
    "! python3 ~/Documentos/ACDC-UD/conll18_ud_eval.py -v \"experimento-obl-arg-test.conllu\" \"experimento-obl-arg-test-udpipe.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric     | Precision |    Recall |  F1 Score | AligndAcc\n",
      "-----------+-----------+-----------+-----------+-----------\n",
      "Tokens     |    100.00 |    100.00 |    100.00 |\n",
      "Sentences  |    100.00 |    100.00 |    100.00 |\n",
      "Words      |    100.00 |    100.00 |    100.00 |\n",
      "UPOS       |     96.52 |     96.52 |     96.52 |     96.52\n",
      "XPOS       |    100.00 |    100.00 |    100.00 |    100.00\n",
      "UFeats     |     95.12 |     95.12 |     95.12 |     95.12\n",
      "AllTags    |     93.32 |     93.32 |     93.32 |     93.32\n",
      "Lemmas     |     96.95 |     96.95 |     96.95 |     96.95\n",
      "UAS        |     85.83 |     85.83 |     85.83 |     85.83\n",
      "LAS        |     81.59 |     81.59 |     81.59 |     81.59\n",
      "CLAS       |     73.80 |     73.53 |     73.67 |     73.53\n",
      "MLAS       |     66.45 |     66.21 |     66.33 |     66.21\n",
      "BLEX       |     70.45 |     70.19 |     70.32 |     70.19\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE BOSQUE2.8\n",
    "#! python3 ~/Documentos/ACDC-UD/tokenizar_conllu.py \"bosque-ud-2.8/pt_bosque-ud-test.conllu\" \"bosque-ud-2.8/pt_bosque-ud-test-tokenizado.txt\"\n",
    "#! python3 ~/Documentos/ACDC-UD/udpipe_vertical.py \"bosque-ud-2.8/bosque2.8.udpipe\" \"bosque-ud-2.8/pt_bosque-ud-test-tokenizado.txt\" \"bosque-ud-2.8/pt_bosque-ud-test-udpipe.conllu\"\n",
    "! python3 ~/Documentos/ACDC-UD/conll18_ud_eval.py -v \"bosque-ud-2.8/pt_bosque-ud-test.conllu\" \"bosque-ud-2.8/pt_bosque-ud-test-udpipe.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Already on 'master'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M\tPetroles_3/prepare_release.ipynb\n",
      "Seu ramo está à frente de 'origin/master' por 45 submissões.\n",
      "  (use \"git push\" to publish your local commits)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 10.53188943862915build: 12.219332456588745build: 11.631117105484009"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences that matter: 8955\n"
     ]
    }
   ],
   "source": [
    "# COUNT MODIFICATIONS\n",
    "file = \"Petroles_3_golden.conllu\"\n",
    "file_stanza = \"Petroles_3_stanza.conllu\"\n",
    "file_udpipe = \"Petroles_3_udpipe.conllu\"\n",
    "post_changes = \"master\"\n",
    "\n",
    "os.system(\"git checkout {}\".format(post_changes))\n",
    "corpus_pos = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_pos.load(file)\n",
    "\n",
    "corpus_stanza = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_stanza.load(file_stanza)\n",
    "corpus_udpipe = estrutura_ud.Corpus(recursivo=True)\n",
    "corpus_udpipe.load(file_udpipe)\n",
    "\n",
    "sentences_that_matter = list(corpus_pos.sentences.keys())\n",
    "print(\"Sentences that matter: {}\".format(len(sentences_that_matter)))\n",
    "\n",
    "tokens_changed = set()\n",
    "tokens_changed_lemma = set()\n",
    "tokens_changed_deprel = set()\n",
    "tokens_changed_dephead = set()\n",
    "tokens_changed_upos = set()\n",
    "sentences_changed = set()\n",
    "udpipe_correct = set()\n",
    "changed_lemmas = {}\n",
    "all_tokens = 0\n",
    "all_sentences = 0\n",
    "cm = set()\n",
    "for sentid in sentences_that_matter:\n",
    "    for t, token in enumerate(corpus_pos.sentences[sentid].tokens):\n",
    "        if not '-' in token.id and token.upos != \"PUNCT\":\n",
    "            all_tokens += 1\n",
    "            try:\n",
    "                token_pos = corpus_pos.sentences[sentid].tokens[t]\n",
    "                token_stanza = corpus_stanza.sentences[sentid].tokens[t]\n",
    "                token_udpipe = corpus_udpipe.sentences[sentid].tokens[t]\n",
    "            except:\n",
    "                continue\n",
    "            important_cols_pos = [token_pos.lemma, token_pos.upos, token_pos.feats, token_pos.dephead, token_pos.deprel]\n",
    "            important_cols_stanza = [token_stanza.lemma, token_stanza.upos, token_stanza.feats, token_stanza.dephead, token_stanza.deprel]\n",
    "            important_cols_udpipe = [token_udpipe.lemma, token_udpipe.upos, token_udpipe.feats, token_udpipe.dephead, token_udpipe.deprel]\n",
    "            if token_stanza.deprel != token_udpipe.deprel:\n",
    "                cm.add(\"{}-{}\".format(sentid, t))\n",
    "                if token_udpipe.deprel == token_pos.deprel:\n",
    "                    udpipe_correct.add(\"{}-{}\".format(sentid, t))\n",
    "            if important_cols_pos != important_cols_stanza:\n",
    "                #if token_pos.deprel == \"obl:arg\" and token_stanza.deprel in [\"obj\", \"iobj\"]:\n",
    "                    #continue\n",
    "                tokens_changed.add(\"{}-{}\".format(sentid, t))\n",
    "                if not sentid in sentences_changed:\n",
    "                    sentences_changed.add(sentid)\n",
    "                if token_pos.lemma != token_stanza.lemma:\n",
    "                    tokens_changed_lemma.add(\"{}-{}\".format(sentid, t))\n",
    "                    if token_pos.lemma.lower() != token_stanza.lemma.lower() and token_pos.upos == \"VERB\":\n",
    "                        if not token_pos.lemma.lower() in changed_lemmas:\n",
    "                            changed_lemmas[token_pos.lemma.lower()] = {}\n",
    "                        if not token_stanza.lemma.lower() in changed_lemmas[token_pos.lemma.lower()]:\n",
    "                            changed_lemmas[token_pos.lemma.lower()][token_stanza.lemma.lower()] = 0\n",
    "                        changed_lemmas[token_pos.lemma.lower()][token_stanza.lemma.lower()] += 1\n",
    "                if token_pos.upos != token_stanza.upos:\n",
    "                    tokens_changed_upos.add(\"{}-{}\".format(sentid, t))\n",
    "                if token_pos.dephead != token_stanza.dephead:\n",
    "                    tokens_changed_dephead.add(\"{}-{}\".format(sentid, t))\n",
    "                if token_pos.deprel != token_stanza.deprel:\n",
    "                    tokens_changed_deprel.add(\"{}-{}\".format(sentid, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aberto (abrir) & absorvar (absorver) & acentuado (acentuar) & adegar (adequar) \\\\\n",
      "adsorvir, adsorvar (adsorver) & adver (advir) & aflor (aflorar) & agar, ager (agir) \\\\\n",
      "aglomer (aglomerar) & agregado (agregar) & agguardar (aguardar) & abustar (ajustar) \\\\\n",
      "alcance (alcançar) & aliado (aliar) & alicerçado (alicerçar) & alinhado (alinhar) \\\\\n",
      "alongado (alongar) & aprofundir (aprofundar) & arranjado (arranjar) & assinalado (assinalar) \\\\\n",
      "associado, associados (associar) & atinjir (atingir) & avaliado (avaliar) & banhado (banhar) \\\\\n",
      "hasear (basear) & bloqueir (bloquear) & bombber (bombear) & quscar (buscar) \\\\\n",
      "captura (capturar) & caracterizado (caracterizar) & carrer (carrear) & causa (causar) \\\\\n",
      "cedir (ceder) & centralizado (centralizar) & cercado (cercar) & chamado (chamar) \\\\\n",
      "cimentado (cimentar) & classificado (classificar) & coleta (coletar) & combinado (combinar) \\\\\n",
      "computado (computar) & concentrado (concentrar) & condicionado (condicionar) & conduzido (conduzir) \\\\\n",
      "conectado (conectar) & a, coferir (conferir) & contido, contida (conter) & contraponder (contrapor) \\\\\n",
      "coordenada (coordenar) & correlacionado (correlacionar) & decaiar (decair) & definido (definir) \\\\\n",
      "demanda (demandar) & demonstrado (demonstrar) & deprender (depreender) & desenvolvar (desenvolver) \\\\\n",
      "desequilibrado (desequilibrar) & desfolha (desfolhar) & deslocado (deslocar) & destacado (destacar) \\\\\n",
      "detalhado (detalhar) & determinado (determinar) & devido (dever) & difundido (difundir) \\\\\n",
      "direcionado (direcionar) & disperso (dispersar) & disposto (dispor) & disococar (dissociar) \\\\\n",
      "disosolver, dissolvido (dissolver) & distorcir (distorcer) & dividido, diver (dividir) & dito (dizer) \\\\\n",
      "dorar (durar) & elever (elevar) & elucidor (elucidar) & embasado (embasar) \\\\\n",
      "emergar (emergir) & empilhado (empilhar) & empregado (empregar) & emulsionado (emulsionar) \\\\\n",
      "encarregado (encarregar) & enfrentir (enfrentar) & entendido, entendidos (entender) & entre (entrar) \\\\\n",
      "envolvido (envolver) & eroder (erodir) & espesso (espessar) & esquumatizar (esquematizar) \\\\\n",
      "estimado (estimar) & estreito (estreitar) & estudo, estudado (estudar) & evidenciado (evidenciar) \\\\\n",
      "exerçar (exercer) & exijar (exigir) & expandido, expandiar, expandar (expandir) & . (explorar) \\\\\n",
      "expresso, esprimir (expressar) & esprimir, expresso (exprimir) & extrar, extragar (extrair) & fervir (ferver) \\\\\n",
      "aormar, former, forma (formar) & generalizado (generalizar) & georreferenciado (georreferenciar) & gerir (gerar) \\\\\n",
      "grar (gradar) & a, ouvir (haver) & identifcar (identificar) & ilustrado (ilustrar) \\\\\n",
      "impecar (impedir) & implantado (implantar) & incluído (incluir) & indicado (indicar) \\\\\n",
      "jnferir (inferir) & inibar (inibir) & injetir (injetar) & inserido (inserir) \\\\\n",
      "instalado (instalar) & intensicarcar (intensificar) & interager (interagir) & interessado (interessar) \\\\\n",
      "interligado (interligar) & interprandar, interpretado (interpretar) & intrudido (intrudir) & invertar (inverter) \\\\\n",
      "investida (investir) & isento (isentar) & justaposto (justapor) & liber (liberar) \\\\\n",
      "ligado (ligar) & listado (listar) & manifesto (manifestar) & mantar, mater (manter) \\\\\n",
      "marca (marcar) & mascar (mascarar) & medida, qedir (medir) & melhora (melhorar) \\\\\n",
      "migrer (migrar) & qinimizar (minimizar) & mistura (misturar) & moidar (moer) \\\\\n",
      "motrar, mostrado (mostrar) & necessitir (necessitar) & norter (nortear) & nota, eotar, eutar (notar) \\\\\n",
      "objetivo, abjetivar (objetivar) & abservar, obervar (observar) & abtevar, obitir, obtir (obter) & orintar (orientar) \\\\\n",
      "parada (parar) & partida (partir) & percorrar (percorrer) & permeier, permer, permermar (permear) \\\\\n",
      "plotado (plotar) & posicionado (posicionar) & possuar (possuir) & prenchher, preenchido (preencher) \\\\\n",
      "presa (prender) & preponder (preponderar) & prevenar (prevenir) & produzido (produzir) \\\\\n",
      "prograr (progradar) & projetado (projetar) & proporar (propor) & protejer (proteger) \\\\\n",
      "providencial (providenciar) & provar (provir) & pré-tratada (pré-tratar) & pulverizado (pulverizar) \\\\\n",
      "posto (pôr) & queima (queimar) & recolhido (recolher) & recomendado (recomendar) \\\\\n",
      "refinado (refinar) & relacionado (relacionar) & removar (remover) & repousado (repousar) \\\\\n",
      "representado, represetar (representar) & restritar (restringir) & resultado (resultar) & retirada (retirar) \\\\\n",
      "retomada (retomar) & satisfaçor (satisfazer) & saturado (saturar) & seco (secar) \\\\\n",
      "seguer (seguir) & separado (separar) & situado (situar) & sobrer (sobressair) \\\\\n",
      "sofrar (sofrer) & sotoposto, sotoposta (sotopor) & subdivider (subdividir) & submetido (submeter) \\\\\n",
      "subordinado (subordinar) & sucedido (suceder) & sugerido (sugerir) & superposto (superpor) \\\\\n",
      "supçar, suposto (supor) & supordado (suportar) & assurgir (surgir) & tangar (tanger) \\\\\n",
      "tomar (tornar) & trabalhhar (trabalhar) & traduzido (traduzir) & transforar (transformar) \\\\\n",
      "tratado, _ (tratar) & trocado (trocar) & fsar, usir (usar) & ftilizar (utilizar) \\\\\n",
      "videre (ver) & visualizado (visualizar) & voltado (voltar) & serar (zerar) \\\\\n",
      "\n",
      "Lemas corrigidos: 212\n",
      "Ocorrências: 621\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\n",
    "ocorrencias = 0\n",
    "c = 1\n",
    "for lemma in sorted(changed_lemmas):\n",
    "    ocorrencias += sum([changed_lemmas[lemma][x] for x in changed_lemmas[lemma]])\n",
    "    txt += \"{} ({}) {}\".format(\", \".join(list(changed_lemmas[lemma].keys())), lemma, \"& \" if c%4 else \"\\\\\\\\\\n\")\n",
    "    c += 1\n",
    "print(txt)\n",
    "\n",
    "print(\"Lemas corrigidos: {}\".format(len(changed_lemmas)))\n",
    "print(\"Ocorrências: {}\".format(ocorrencias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAY INTERSEÇÕES\n",
    "print(\"EXCEPT PUNCT -- Tokens changed: {} / All_tokens: {}\".format(len(tokens_changed), all_tokens))\n",
    "print(\"Sentences changed: {} / All_sentences: {}\".format(len(sentences_changed), len(sentences_that_matter)))\n",
    "print(\"CM: {} / Intersection: {} / UDPIPE_CORRECT: {}\".format(len(cm), len(tokens_changed.intersection(cm)), len(udpipe_correct)))\n",
    "print(len(tokens_changed_lemma))\n",
    "print(len(tokens_changed_upos))\n",
    "print(len(tokens_changed_dephead))\n",
    "print(len(tokens_changed_deprel))\n",
    "print(\"\")\n",
    "print(len(tokens_changed_lemma.intersection(cm)))\n",
    "print(len(tokens_changed_upos.intersection(cm)))\n",
    "print(len(tokens_changed_dephead.intersection(cm)))\n",
    "print(len(tokens_changed_deprel.intersection(cm)))\n",
    "print(\"\")\n",
    "print(len(tokens_changed_lemma.intersection(tokens_changed_upos)))\n",
    "print(len(tokens_changed_lemma.intersection(tokens_changed_upos).intersection(cm)))\n",
    "print(len(tokens_changed_lemma.intersection(tokens_changed_deprel)))\n",
    "print(len(tokens_changed_lemma.intersection(tokens_changed_deprel).intersection(cm)))\n",
    "print(len(tokens_changed_upos.intersection(tokens_changed_deprel)))\n",
    "print(len(tokens_changed_upos.intersection(tokens_changed_deprel).intersection(cm)))\n",
    "print(len(tokens_changed_dephead.intersection(tokens_changed_deprel)))\n",
    "print(len(tokens_changed_dephead.intersection(tokens_changed_deprel).intersection(cm)))\n",
    "print(len(tokens_changed_lemma - tokens_changed_upos - tokens_changed_dephead - tokens_changed_deprel))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
