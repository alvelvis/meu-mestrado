{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "    ! pip3 install sklearn\n",
    "import os\n",
    "if not os.path.isfile(\"estrutura_ud.py\"):\n",
    "    ! wget https://raw.githubusercontent.com/alvelvis/ACDC-UD/master/estrutura_ud.py\n",
    "import estrutura_ud\n",
    "import itertools\n",
    "import json\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotators = [\"Aline\", \"Elvis\", \"Tati\", \"Wogue\", \"Maria_Clara\"]\n",
    "attributes = [\"lemma\", \"feats\", \"dephead\", \"deprel\", \"upos\"]\n",
    "file_name = \"50R\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "build: 0.022608280181884766build: 0.02207660675048828build: 0.02124786376953125build: 0.023970365524291992"
     ]
    }
   ],
   "source": [
    "# Load files\n",
    "annotators_files = {}\n",
    "system_file = 0\n",
    "golden_file = 0\n",
    "for annotator in annotators:\n",
    "    if os.path.isfile(\"{}_{}.conllu\".format(file_name, annotator)):\n",
    "        corpus = estrutura_ud.Corpus()\n",
    "        corpus.load(\"{}_{}.conllu\".format(file_name, annotator))\n",
    "        annotators_files[annotator] = corpus\n",
    "if os.path.isfile(\"{}_golden.conllu\".format(file_name)):\n",
    "    corpus = estrutura_ud.Corpus()\n",
    "    corpus.load(\"{}_golden.conllu\".format(file_name))\n",
    "    golden_file = corpus\n",
    "if os.path.isfile(\"{}_system.conllu\".format(file_name)):\n",
    "    corpus = estrutura_ud.Corpus()\n",
    "    corpus.load(\"{}_system.conllu\".format(file_name))\n",
    "    system_file = corpus\n",
    "has_system_and_golden = system_file and golden_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove sentences with tokenization issues\n",
    "if has_system_and_golden:\n",
    "    golden_sentences = set([x for x in golden_file.sentences])\n",
    "    for annotator in annotators_files:\n",
    "        for sent_id in list(annotators_files[annotator].sentences.keys()):\n",
    "            if not sent_id in golden_sentences:\n",
    "                del annotators_files[annotator].sentences[sent_id]\n",
    "                print(\"Not exist anymore: {} Annotator {} sent_id {}\".format(file_name, annotator, sent_id))\n",
    "            else:\n",
    "                if sent_id not in golden_file.sentences or sent_id not in system_file.sentences:\n",
    "                    del annotators_files[annotator].sentences[sent_id]\n",
    "                    print(\"Not in golden or system: {} Annotator {} sent_id {}\".format(file_name, annotator, sent_id))\n",
    "                else:\n",
    "                    if len(golden_file.sentences[sent_id].tokens) != len(system_file.sentences[sent_id].tokens):\n",
    "                        del annotators_files[annotator].sentences[sent_id]\n",
    "                        print(\"Different n. of tokens: {} Annotator {} sent_id {}\".format(file_name, annotator, sent_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n    \"tokens\": 762,\n    \"sentences\": 20,\n    \"tokens_changed\": 0\n}\n"
     ]
    }
   ],
   "source": [
    "# Experiment\n",
    "statistics = {'tokens': 0, 'sentences': 0, 'tokens_changed': 0}\n",
    "people = {annotator: {'tokens_changed': 0, 'correct_changes': 0} for annotator in annotators_files}\n",
    "correctedness = {annotator: 0 for annotator in annotators_files}\n",
    "\n",
    "def have_same_attributes(token_1, token_2, attributes=attributes):\n",
    "    return [token_1.__dict__[x] for x in attributes] == [token_2.__dict__[x] for x in attributes]\n",
    "\n",
    "for i, annotator in enumerate(annotators_files):\n",
    "    if i == 0:\n",
    "        for sent_id, sentence in annotators_files[annotator].sentences.items():\n",
    "            statistics['sentences'] += 1\n",
    "            for t, token in enumerate(sentence.tokens):\n",
    "                if not '-' in token.id:\n",
    "                    statistics['tokens'] += 1\n",
    "                    if has_system_and_golden:\n",
    "                        if not have_same_attributes(golden_file.sentences[sent_id].tokens[t], system_file.sentences[sent_id].tokens[t]):\n",
    "                            statistics['tokens_changed'] += 1\n",
    "    if has_system_and_golden:\n",
    "        for sent_id, sentence in annotators_files[annotator].sentences.items():\n",
    "            for t, token in enumerate(sentence.tokens):\n",
    "                if not '-' in token.id:\n",
    "                    if not have_same_attributes(system_file.sentences[sent_id].tokens[t], token):\n",
    "                        people[annotator]['tokens_changed'] += 1\n",
    "                        if have_same_attributes(token, golden_file.sentences[sent_id].tokens[t]):\n",
    "                            people[annotator]['correct_changes'] += 1\n",
    "\n",
    "kappa = {}\n",
    "kappa_changed = {}\n",
    "for combination in itertools.combinations(list(annotators_files.keys()), r=2):\n",
    "    kappa[combination] = {}\n",
    "    kappa_changed[combination] = {}\n",
    "    tags = {}\n",
    "    tags_changed = {}\n",
    "    for annotator in combination:\n",
    "        tags[annotator] = {}\n",
    "        tags_changed[annotator] = {}\n",
    "        for attribute in attributes:\n",
    "            tags[annotator][attribute] = []\n",
    "            tags_changed[annotator][attribute] = []\n",
    "        for sent_id, sentence in annotators_files[annotator].sentences.items():\n",
    "            for t, token in enumerate(sentence.tokens):\n",
    "                if not '-' in token.id:\n",
    "                    for attribute in attributes:\n",
    "                        tags[annotator][attribute].append(token.__dict__[attribute])\n",
    "                        if has_system_and_golden:\n",
    "                            if not have_same_attributes(golden_file.sentences[sent_id].tokens[t], system_file.sentences[sent_id].tokens[t]):\n",
    "                                tags_changed[annotator][attribute].append(token.__dict__[attribute])\n",
    "    for attribute in attributes:\n",
    "        kappa[combination][attribute] = cohen_kappa_score(tags[combination[0]][attribute], tags[combination[1]][attribute])\n",
    "        if has_system_and_golden:\n",
    "            kappa_changed[combination][attribute] = cohen_kappa_score(tags_changed[combination[0]][attribute], tags_changed[combination[1]][attribute])\n",
    "\n",
    "print(json.dumps(statistics, ensure_ascii=False, indent=4))\n",
    "if has_system_and_golden:\n",
    "    for annotator in annotators_files:\n",
    "        people[annotator]['_precision'] = people[annotator]['correct_changes'] / people[annotator]['tokens_changed'] if people[annotator]['tokens_changed'] else 0\n",
    "        people[annotator]['_recall'] = people[annotator]['correct_changes'] / statistics['tokens_changed'] if people[annotator]['tokens_changed'] else 0\n",
    "        people[annotator]['_F1'] = 2 * (people[annotator]['_precision'] * people[annotator]['_recall']) / (people[annotator]['_precision'] + people[annotator]['_recall']) if people[annotator]['tokens_changed'] else 0\n",
    "    print(json.dumps(people, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{('Aline', 'Elvis'): {'dephead': 0.9301881839308084,\n                      'deprel': 0.9455555577011832,\n                      'feats': 0.9841041035194182,\n                      'lemma': 0.9904510765363939,\n                      'upos': 0.9744296910883531},\n ('Aline', 'Tati'): {'dephead': 0.9436154206644797,\n                     'deprel': 0.9529821909250076,\n                     'feats': 0.9920504763454725,\n                     'lemma': 0.9945434625382208,\n                     'upos': 0.9856432285778883},\n ('Aline', 'Wogue'): {'dephead': 0.9449616923372343,\n                      'deprel': 0.9471267823894716,\n                      'feats': 0.9872786121667053,\n                      'lemma': 0.99045109363084,\n                      'upos': 0.976070294295602},\n ('Elvis', 'Tati'): {'dephead': 0.9261551147846587,\n                     'deprel': 0.9381683504025317,\n                     'feats': 0.9825089677536482,\n                     'lemma': 0.9849947637418212,\n                     'upos': 0.9696603925414607},\n ('Elvis', 'Wogue'): {'dephead': 0.9382429582489838,\n                      'deprel': 0.9573232084264357,\n                      'feats': 0.9825059012841946,\n                      'lemma': 0.9918152084597661,\n                      'upos': 0.972853011442223},\n ('Tati', 'Wogue'): {'dephead': 0.9422714050885591,\n                     'deprel': 0.9529779294784646,\n                     'feats': 0.9809120627246652,\n                     'lemma': 0.984994790603753,\n                     'upos': 0.9713082604848478}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{('Aline', 'Elvis'): {},\n ('Aline', 'Tati'): {},\n ('Aline', 'Wogue'): {},\n ('Elvis', 'Tati'): {},\n ('Elvis', 'Wogue'): {},\n ('Tati', 'Wogue'): {}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(kappa_changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}