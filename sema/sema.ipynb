{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isfile(\"estrutura_ud.py\"):\n",
    "    ! wget https://raw.githubusercontent.com/alvelvis/ACDC-UD/master/estrutura_ud.py\n",
    "import estrutura_ud\n",
    "try:\n",
    "    import stanza\n",
    "except Exception:\n",
    "    ! pip3 install stanza\n",
    "    import stanza\n",
    "    stanza.download('pt')\n",
    "nlp = stanza.Pipeline('pt')\n",
    "import pprint\n",
    "import shutil\n",
    "import unicodedata\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "tagset_folder = \"tagset\"\n",
    "conllu_folder = \"conllu\"\n",
    "\n",
    "for folder in [tagset_folder, conllu_folder]:\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "col_sema = 8\n",
    "files = {}\n",
    "tags = {}\n",
    "lemmas = {\n",
    "    'ums': 'ums',\n",
    "    'santos': 'santos',\n",
    "    'estai': 'estai',\n",
    "    'veio': 'veio'\n",
    "}\n",
    "\n",
    "if not all(os.path.isdir(x) for x in [tagset_folder, conllu_folder]):\n",
    "    raise Exception(\"Folder not found.\")\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "def stanza_lemmatize(string):\n",
    "    doc = nlp(string)\n",
    "    return \" \".join([token['lemma'] for sentence in doc.to_dict() for token in sentence if 'lemma' in token])\n",
    "\n",
    "def count_tokens(string):\n",
    "    return {\n",
    "        'tokens': len(list(filter(lambda x: len(x.split(\"\\t\")) == 10 and not '-' in x.split(\"\\t\")[0], string.splitlines()))),\n",
    "        'sentences': len([x for x in string.split(\"\\n\\n\") if x.strip()]),\n",
    "        'types': {x.split(\"\\t\")[1] for x in filter(lambda x: len(x.split(\"\\t\")) == 10, string.splitlines())},\n",
    "        }\n",
    "\n",
    "def tag_sentence(sentence, entity, tag):\n",
    "    tokens = list(filter(lambda x: isinstance(x, list) and not '-' in x[0], sentence))\n",
    "    metadata = list(filter(lambda x: isinstance(x, str), sentence))\n",
    "    mwt = {l: line for l, line in enumerate(sentence) if isinstance(line, list) and '-' in line[0]}\n",
    "    ngram_span = len(entity)\n",
    "    for t, token in enumerate(tokens):\n",
    "        if t + ngram_span > len(tokens):\n",
    "            break\n",
    "        ngram_word = [remove_accents(tokens[t+i][1].lower()) for i in range(ngram_span)]\n",
    "        ngram_lemma = [remove_accents(tokens[t+i][2].lower()) for i in range(ngram_span)]\n",
    "        if any(x == entity for x in [ngram_word, ngram_lemma]):\n",
    "            if entity[0] == \"campos\" and tokens[t][0][0] != \"C\":\n",
    "                continue\n",
    "            for i in range(ngram_span):\n",
    "                tokens[t+i][col_sema] = \"|\".join(set([x for x in tokens[t+i][col_sema].split(\"|\") if x != \"O\"] + [\"{}={}\".format(\"I\" if i else \"B\", tag)]))\n",
    "    sentence = metadata + tokens\n",
    "    for line in mwt:\n",
    "        sentence.insert(line, mwt[line])\n",
    "    return sentence\n",
    "\n",
    "def count_tags(text):\n",
    "    frequency = []\n",
    "    for token in filter(lambda x: len(x.split(\"\\t\")) == 10 and not '-' in x.split(\"\\t\")[0] and \"B=\" in x.split(\"\\t\")[col_sema], text.splitlines()):\n",
    "        frequency.extend([x.replace(\"B=\", \"\") for x in token.split(\"\\t\")[col_sema].split(\"|\") if x.startswith(\"B=\")])\n",
    "    return frequency\n",
    "\n",
    "def apply_rules(text):\n",
    "    corpus = estrutura_ud.Corpus()\n",
    "    corpus.build(text)\n",
    "    for sentence in corpus.sentences.values():\n",
    "        for token in sentence.tokens:\n",
    "            pass\n",
    "    return corpus.to_str()\n",
    "\n",
    "# Build tagset lexicon\n",
    "for file in os.listdir(tagset_folder):\n",
    "    if file.lower().endswith(\".txt\"):\n",
    "        tag = file.upper().split(\".TXT\")[0].replace(\"_\", \":\").replace(\" \", \"_\")\n",
    "        with open(\"{}/{}\".format(tagset_folder, file)) as f:\n",
    "            text = f.read()\n",
    "        tags[tag] = set([x.strip().lower() for x in text.splitlines() if not x.strip().startswith(\"#\") and x.strip()])\n",
    "\n",
    "# Load conllu files and gather initial statistics\n",
    "for file in os.listdir(conllu_folder):\n",
    "    if file.lower().endswith(\".conllu\"):\n",
    "        with open(\"{}/{}\".format(conllu_folder, file)) as f:\n",
    "            text = f.read()\n",
    "        sentences = [x.splitlines() for x in text.split(\"\\n\\n\") if x.strip()]\n",
    "        for s, sentence in enumerate(sentences):\n",
    "            for l, line in enumerate(sentence):\n",
    "                if len(line.split(\"\\t\")) == 10:\n",
    "                    sentences[s][l] = line.split(\"\\t\")\n",
    "                    sentences[s][l][col_sema] = \"O\"\n",
    "        files[file] = {'text': text, 'tagged': sentences, 'tags_frequency': []}\n",
    "        files[file].update(count_tokens(files[file]['text']))\n",
    "\n",
    "# Tag each file\n",
    "for f, file in enumerate(files):\n",
    "    text_normalized = remove_accents(files[file]['text'].lower())\n",
    "    for t, tag in enumerate(tags):\n",
    "        for e, entity in enumerate(list(filter(lambda x: remove_accents(x) in text_normalized, tags[tag]))):\n",
    "            if not entity in lemmas:\n",
    "                lemmas[entity] = stanza_lemmatize(entity)\n",
    "            print(\"{}/{} - {}/{} - {} / {}: {} - {} {}\".format(f+1, len(files), t+1, len(tags), e+1, len(tags[tag]), entity, lemmas[entity], \"-- skip lemma\" if lemmas[entity] == entity else \"\"))\n",
    "            for ngram in [entity, lemmas[entity] if lemmas[entity] != entity else \"\"]:\n",
    "                if ngram:\n",
    "                    for s, sentence in enumerate(files[file]['tagged']):\n",
    "                        files[file]['tagged'][s] = tag_sentence(sentence, [remove_accents(x) for x in ngram.split(\" \")], tag)        \n",
    "    files[file]['output'] = files[file]['tagged'][:]\n",
    "    for s, sentence in enumerate(files[file]['tagged']):\n",
    "        for t, token in enumerate(sentence):\n",
    "            if isinstance(token, list):\n",
    "                files[file]['output'][s][t] = \"\\t\".join(files[file]['output'][s][t])\n",
    "        files[file]['output'][s] = \"\\n\".join(files[file]['output'][s])\n",
    "    files[file]['output'] = \"\\n\\n\".join(files[file]['output'])\n",
    "    files[file]['output'] = apply_rules(files[file]['output'])\n",
    "    files[file]['tags_frequency'].extend(count_tags(files[file]['output']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Stats\n",
    "all_tags = []\n",
    "[all_tags.extend(files[x]['tags_frequency']) for x in files]\n",
    "all_tags = set(all_tags)\n",
    "\n",
    "stats = \"File;%NE;NE;TTR;SENT;TOKENS\"\n",
    "for file in sorted(files, reverse=True, key=lambda x: sum([files[x]['tags_frequency'].count(y) for y in set(files[x]['tags_frequency'])]) / files[x]['tokens']):\n",
    "    stats += \"\\n{};{};{};{};{};{}\".format(\n",
    "        file, \n",
    "        sum([files[file]['tags_frequency'].count(y) for y in set(files[file]['tags_frequency'])]) / files[file]['tokens'],\n",
    "        sum([files[file]['tags_frequency'].count(y) for y in set(files[file]['tags_frequency'])]),\n",
    "        len(files[file]['types']) / files[file]['tokens'],\n",
    "        files[file]['sentences'],\n",
    "        files[file]['tokens']\n",
    "        )\n",
    "with open(\"files_stats.csv\", \"w\") as f:\n",
    "    f.write(stats)\n",
    "print(stats + \"\\n\")\n",
    "\n",
    "stats = \"Tag;NE;FILES\"\n",
    "for tag in sorted(all_tags, reverse=True, key=lambda x: sum([files[y]['tags_frequency'].count(x) for y in files])):\n",
    "    stats += \"\\n{};{};{}\".format(\n",
    "        tag,\n",
    "        sum([files[y]['tags_frequency'].count(tag) for y in files]),\n",
    "        len(list(filter(lambda x: tag in files[x]['tags_frequency'], files)))\n",
    "    )\n",
    "with open(\"tags_stats.csv\", \"w\") as f:\n",
    "    f.write(stats)\n",
    "print(stats + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tagged files\n",
    "if os.path.isdir(\"tagged\"):\n",
    "    shutil.rmtree(\"tagged\")\n",
    "os.mkdir(\"tagged\")\n",
    "for file in files:\n",
    "    with open(\"tagged/{}\".format(file), \"w\") as f:\n",
    "        f.write(files[file]['output'])\n",
    "    #os.system(\"meld --diff {}/{} tagged/{}\".format(conllu_folder, file, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find from which word a lemma came\n",
    "def GetKey(dictA, val):\n",
    "    for key, value in dictA.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    return \"key doesn't exist\"\n",
    "\n",
    "GetKey(lemmas, \"presen√ßa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}